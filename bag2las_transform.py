import os
import rosbag
import laspy
import sensor_msgs.point_cloud2 as pc2
import numpy as np
from collections import defaultdict
import time
from scipy.spatial.distance import cdist
from scipy.interpolate import interp1d
from scipy.spatial import KDTree
from sklearn.neighbors import NearestNeighbors
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from scipy.optimize import least_squares
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# TRANSFORMATION FUNCTIONS
# ============================================================================

def slerp_quaternions(q1, q2, t):
    """
    Spherical Linear Interpolation (SLERP) for quaternions.
    
    Parameters:
        q1: Starting quaternion [x, y, z, w]
        q2: Ending quaternion [x, y, z, w]
        t: Interpolation parameter [0, 1]
        
    Returns:
        np.array: Interpolated quaternion
    """
    # Ensure inputs are numpy arrays
    q1 = np.array(q1, dtype=np.float64)
    q2 = np.array(q2, dtype=np.float64)
    
    # Normalize quaternions
    q1 = q1 / np.linalg.norm(q1)
    q2 = q2 / np.linalg.norm(q2)
    
    # Compute the cosine of the angle between them
    dot = np.dot(q1, q2)
    
    # If the dot product is negative, slerp won't take the shorter path
    # So we negate one quaternion to correct this
    if dot < 0.0:
        q2 = -q2
        dot = -dot
    
    # If the inputs are too close for comfort, linearly interpolate
    DOT_THRESHOLD = 0.9995
    if dot > DOT_THRESHOLD:
        result = q1 + t * (q2 - q1)
        return result / np.linalg.norm(result)
    
    # Calculate the angle between the quaternions
    theta_0 = np.arccos(np.abs(dot))  # theta_0 = angle between input vectors
    sin_theta_0 = np.sin(theta_0)     # compute this only once
    theta = theta_0 * t               # theta = angle between v0 and result
    sin_theta = np.sin(theta)         # compute this only once
    
    # q2_perp = normalize(q2 - q1 * dot(q1, q2))
    s0 = np.cos(theta) - dot * sin_theta / sin_theta_0  # == sin(theta_0 - theta) / sin(theta_0)
    s1 = sin_theta / sin_theta_0
    
    return s0 * q1 + s1 * q2

def quaternion_to_rotation_matrix(quaternion):
    """
    Convert quaternion to 3x3 rotation matrix.
    
    Parameters:
        quaternion: [x, y, z, w] quaternion array
        
    Returns:
        np.array: 3x3 rotation matrix
    """
    x, y, z, w = quaternion
    
    # Normalize quaternion
    norm = np.sqrt(x*x + y*y + z*z + w*w)
    if norm > 0:
        x, y, z, w = x/norm, y/norm, z/norm, w/norm
    
    # Calculate rotation matrix elements
    rotation_matrix = np.array([
        [1 - 2*(y*y + z*z), 2*(x*y - z*w), 2*(x*z + y*w)],
        [2*(x*y + z*w), 1 - 2*(x*x + z*z), 2*(y*z - x*w)],
        [2*(x*z - y*w), 2*(y*z + x*w), 1 - 2*(x*x + y*y)]
    ])
    
    return rotation_matrix

def create_transform_matrix(position, quaternion, lidar_offset=None):
    """
    Create 4x4 homogeneous transformation matrix from position and quaternion.
    
    Parameters:
        position: [x, y, z] position array
        quaternion: [x, y, z, w] quaternion array
        lidar_offset: Optional [x, y, z, rx, ry, rz] offset from robot base to lidar
        
    Returns:
        np.array: 4x4 transformation matrix
    """
    transform_matrix = np.eye(4)
    
    # Set rotation part
    transform_matrix[:3, :3] = quaternion_to_rotation_matrix(quaternion)
    
    # Set translation part
    transform_matrix[:3, 3] = position
    
    # Apply lidar offset if provided
    if lidar_offset is not None:
        # Create lidar offset matrix
        lidar_trans = lidar_offset[:3]  # Translation offset
        if len(lidar_offset) > 3:
            # Rotation offset (Euler angles in radians)
            rx, ry, rz = lidar_offset[3:6]
            # Create rotation matrix from Euler angles (ZYX convention)
            cos_rx, sin_rx = np.cos(rx), np.sin(rx)
            cos_ry, sin_ry = np.cos(ry), np.sin(ry)
            cos_rz, sin_rz = np.cos(rz), np.sin(rz)
            
            # ZYX Euler angle rotation matrix
            R_lidar = np.array([
                [cos_ry*cos_rz, -cos_ry*sin_rz, sin_ry],
                [sin_rx*sin_ry*cos_rz + cos_rx*sin_rz, -sin_rx*sin_ry*sin_rz + cos_rx*cos_rz, -sin_rx*cos_ry],
                [-cos_rx*sin_ry*cos_rz + sin_rx*sin_rz, cos_rx*sin_ry*sin_rz + sin_rx*cos_rz, cos_rx*cos_ry]
            ])
        else:
            R_lidar = np.eye(3)
        
        # Create lidar offset transformation matrix
        lidar_offset_matrix = np.eye(4)
        lidar_offset_matrix[:3, :3] = R_lidar
        lidar_offset_matrix[:3, 3] = lidar_trans
        
        # Apply offset: T_world_lidar = T_world_robot * T_robot_lidar
        transform_matrix = transform_matrix @ lidar_offset_matrix
    
    return transform_matrix

def apply_transform_to_points(points, transform_matrix):
    """
    Apply transformation matrix to point cloud.
    
    Parameters:
        points: Nx3 array of points [x, y, z]
        transform_matrix: 4x4 transformation matrix
        
    Returns:
        np.array: Transformed Nx3 points
    """
    # Validate inputs
    if points.shape[1] != 3:
        raise ValueError(f"Points must have 3 columns (x, y, z), got {points.shape[1]}")
    
    if transform_matrix.shape != (4, 4):
        raise ValueError(f"Transform matrix must be 4x4, got {transform_matrix.shape}")
    
    # Check for valid transformation matrix
    det = np.linalg.det(transform_matrix[:3, :3])
    if abs(det - 1.0) > 1e-3:
        print(f"‚ö†Ô∏è  Warning: Transformation matrix determinant is {det:.6f}, expected ~1.0")
        print("   This may indicate scaling or reflection in the transformation")
    
    # Convert to homogeneous coordinates
    num_points = points.shape[0]
    homogeneous_points = np.ones((num_points, 4))
    homogeneous_points[:, :3] = points
    
    # Apply transformation
    transformed_homogeneous = (transform_matrix @ homogeneous_points.T).T
    
    # Convert back to 3D coordinates
    transformed_points = transformed_homogeneous[:, :3]
    
    # Validate output
    if np.any(np.isnan(transformed_points)) or np.any(np.isinf(transformed_points)):
        nan_count = np.isnan(transformed_points).sum()
        inf_count = np.isinf(transformed_points).sum()
        print(f"‚ö†Ô∏è  Warning: Transformation produced {nan_count} NaN and {inf_count} inf values")
    
    return transformed_points

def validate_coordinate_systems(points_before, points_after, transform_description=""):
    """
    Validate coordinate system transformation.
    
    Parameters:
        points_before: Original points
        points_after: Transformed points
        transform_description: Description of the transformation
    """
    print(f"   üîç Validating transformation: {transform_description}")
    
    # Check point count consistency
    if len(points_before) != len(points_after):
        print(f"   ‚ùå Point count mismatch: {len(points_before)} -> {len(points_after)}")
        return False
    
    # Calculate transformation statistics
    differences = points_after - points_before
    translation_distances = np.linalg.norm(differences, axis=1)
    
    print(f"   ‚Ä¢ Mean translation distance: {np.mean(translation_distances):.3f} m")
    print(f"   ‚Ä¢ Max translation distance: {np.max(translation_distances):.3f} m")
    print(f"   ‚Ä¢ Min translation distance: {np.min(translation_distances):.3f} m")
    
    # Check for reasonable bounds
    if np.max(translation_distances) > 1000:  # 1km threshold
        print(f"   ‚ö†Ô∏è  Warning: Large transformation distances detected (max: {np.max(translation_distances):.1f} m)")
        print("   This may indicate coordinate system mismatch or incorrect odometry data")
    
    # Check coordinate ranges
    for axis, name in enumerate(['X', 'Y', 'Z']):
        before_range = np.max(points_before[:, axis]) - np.min(points_before[:, axis])
        after_range = np.max(points_after[:, axis]) - np.min(points_after[:, axis])
        range_change = abs(after_range - before_range) / max(before_range, 1e-6)
        
        if range_change > 0.1:  # 10% change threshold
            print(f"   ‚ö†Ô∏è  Warning: {name} axis range changed by {range_change*100:.1f}% ({before_range:.3f} -> {after_range:.3f})")
    
    # Check for height (Z) issues specifically
    z_before_mean = np.mean(points_before[:, 2])
    z_after_mean = np.mean(points_after[:, 2])
    z_change = abs(z_after_mean - z_before_mean)
    
    if z_change > 2.0:  # 2 meter threshold
        print(f"   ‚ö†Ô∏è  Warning: Significant Z (height) change: {z_before_mean:.3f} -> {z_after_mean:.3f} m")
        print("   This may indicate incorrect coordinate system assumptions")
    
    return True

def interpolate_odometry_data(pc_timestamps, odom_timestamps, odom_positions, odom_orientations):
    """
    Interpolate odometry data to match pointcloud timestamps.
    
    Parameters:
        pc_timestamps: Array of pointcloud timestamps
        odom_timestamps: Array of odometry timestamps  
        odom_positions: Nx3 array of odometry positions
        odom_orientations: Nx4 array of odometry quaternions
        
    Returns:
        tuple: (interpolated_positions, interpolated_orientations)
    """
    print("üîÑ –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –æ–¥–æ–º–µ—Ç—Ä–∏–∏...")
    
    # Check if we have enough odometry data
    if len(odom_timestamps) < 2:
        print("‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –æ–¥–æ–º–µ—Ç—Ä–∏–∏ –¥–ª—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏")
        return None, None
    
    # Find valid time range overlap
    odom_start = np.min(odom_timestamps)
    odom_end = np.max(odom_timestamps)
    pc_start = np.min(pc_timestamps)
    pc_end = np.max(pc_timestamps)
    
    print(f"   ‚Ä¢ –î–∏–∞–ø–∞–∑–æ–Ω –≤—Ä–µ–º–µ–Ω–∏ –æ–¥–æ–º–µ—Ç—Ä–∏–∏: {odom_start:.3f} - {odom_end:.3f}")
    print(f"   ‚Ä¢ –î–∏–∞–ø–∞–∑–æ–Ω –≤—Ä–µ–º–µ–Ω–∏ –æ–±–ª–∞–∫–∞ —Ç–æ—á–µ–∫: {pc_start:.3f} - {pc_end:.3f}")
    
    # Check overlap
    overlap_start = max(odom_start, pc_start)
    overlap_end = min(odom_end, pc_end)
    
    if overlap_start >= overlap_end:
        print("‚ùå –ù–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É –¥–∞–Ω–Ω—ã–º–∏ –æ–¥–æ–º–µ—Ç—Ä–∏–∏ –∏ –æ–±–ª–∞–∫–æ–º —Ç–æ—á–µ–∫")
        return None, None
    
    print(f"   ‚Ä¢ –û–±–ª–∞—Å—Ç—å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è: {overlap_start:.3f} - {overlap_end:.3f}")
    
    try:
        # Interpolate positions
        interp_x = interp1d(odom_timestamps, odom_positions[:, 0], 
                           kind='linear', bounds_error=False, fill_value='extrapolate')
        interp_y = interp1d(odom_timestamps, odom_positions[:, 1], 
                           kind='linear', bounds_error=False, fill_value='extrapolate')
        interp_z = interp1d(odom_timestamps, odom_positions[:, 2], 
                           kind='linear', bounds_error=False, fill_value='extrapolate')
        
        interpolated_positions = np.column_stack([
            interp_x(pc_timestamps),
            interp_y(pc_timestamps), 
            interp_z(pc_timestamps)
        ])
        
        # For quaternions, we need special handling - use SLERP instead of linear interpolation
        print("   üîÑ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SLERP –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏ –¥–ª—è –∫–≤–∞—Ç–µ—Ä–Ω–∏–æ–Ω–æ–≤...")
        interpolated_orientations = np.zeros((len(pc_timestamps), 4))
        
        for i, pc_time in enumerate(pc_timestamps):
            # Find the closest odometry timestamps
            if pc_time <= odom_timestamps[0]:
                # Use first quaternion
                interpolated_orientations[i] = odom_orientations[0]
            elif pc_time >= odom_timestamps[-1]:
                # Use last quaternion
                interpolated_orientations[i] = odom_orientations[-1]
            else:
                # Find surrounding points for SLERP
                idx = np.searchsorted(odom_timestamps, pc_time)
                if idx == 0:
                    idx = 1
                
                t1, t2 = odom_timestamps[idx-1], odom_timestamps[idx]
                q1, q2 = odom_orientations[idx-1], odom_orientations[idx]
                
                # Calculate interpolation parameter
                t = (pc_time - t1) / (t2 - t1) if t2 != t1 else 0.0
                t = np.clip(t, 0.0, 1.0)
                
                # Apply SLERP
                interpolated_orientations[i] = slerp_quaternions(q1, q2, t)
        
        print(f"   ‚úÖ –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–ª—è {len(pc_timestamps)} –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–æ—á–µ–∫")
        
        return interpolated_positions, interpolated_orientations
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏: {e}")
        return None, None

def collect_odometry_data(bag_file, odometry_topic):
    """
    Collect all odometry data from bag file.
    
    Parameters:
        bag_file: Path to bag file
        odometry_topic: Name of odometry topic
        
    Returns:
        tuple: (timestamps, positions, orientations)
    """
    print(f"üß≠ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ–¥–æ–º–µ—Ç—Ä–∏–∏ –∏–∑ —Ç–æ–ø–∏–∫–∞ '{odometry_topic}'...")
    
    timestamps = []
    positions = []
    orientations = []
    
    try:
        with rosbag.Bag(bag_file, 'r') as bag:
            total_messages = bag.get_message_count(topic_filters=odometry_topic)
            print(f"   üìä –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π –æ–¥–æ–º–µ—Ç—Ä–∏–∏: {total_messages:,}")
            
            if total_messages == 0:
                print("‚ùå –ù–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –æ–¥–æ–º–µ—Ç—Ä–∏–∏ –≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–º —Ç–æ–ø–∏–∫–µ")
                return None, None, None
            
            message_count = 0
            for topic, msg, ros_timestamp in bag.read_messages(topics=[odometry_topic]):
                message_count += 1
                
                # Extract timestamp
                timestamps.append(ros_timestamp.to_sec())
                
                # Extract position
                pos = msg.pose.pose.position
                positions.append([pos.x, pos.y, pos.z])
                
                # Extract orientation (quaternion)
                orient = msg.pose.pose.orientation
                orientations.append([orient.x, orient.y, orient.z, orient.w])
                
                # Progress indicator
                if message_count % max(1, total_messages // 10) == 0:
                    progress = (message_count / total_messages) * 100
                    print(f"   üìä [{progress:5.1f}%] –ó–∞–≥—Ä—É–∂–µ–Ω–æ {message_count:,}/{total_messages:,} —Å–æ–æ–±—â–µ–Ω–∏–π")
            
            # Convert to numpy arrays
            timestamps = np.array(timestamps)
            positions = np.array(positions)
            orientations = np.array(orientations)
            
            print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –æ–¥–æ–º–µ—Ç—Ä–∏–∏:")
            print(f"      ‚Ä¢ –í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω: {np.min(timestamps):.3f} - {np.max(timestamps):.3f}")
            print(f"      ‚Ä¢ –û–±—â–∞—è –¥–∏—Å—Ç–∞–Ω—Ü–∏—è: {calculate_total_distance(positions):.2f} –º")
            
            return timestamps, positions, orientations
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –æ–¥–æ–º–µ—Ç—Ä–∏–∏: {e}")
        return None, None, None

def calculate_total_distance(positions):
    """Calculate total distance traveled."""
    if len(positions) < 2:
        return 0.0
    distances = np.sqrt(np.sum(np.diff(positions, axis=0)**2, axis=1))
    return np.sum(distances)

# ============================================================================
# SLAM FUNCTIONS
# ============================================================================

class SimpleSLAM:
    """
    Simplified SLAM implementation for pose graph optimization and loop closure detection.
    """
    
    def __init__(self, loop_closure_distance=2.0, min_time_gap=10.0):
        """
        Initialize SLAM parameters.
        
        Parameters:
            loop_closure_distance: Maximum distance to consider for loop closure
            min_time_gap: Minimum time gap between poses to consider for loop closure
        """
        self.loop_closure_distance = loop_closure_distance
        self.min_time_gap = min_time_gap
        self.poses = []  # List of poses [(timestamp, position, quaternion)]
        self.odometry_edges = []  # Odometry constraints
        self.loop_closure_edges = []  # Loop closure constraints
        
    def add_pose(self, timestamp, position, quaternion):
        """Add a pose to the graph."""
        pose_id = len(self.poses)
        self.poses.append((timestamp, np.array(position), np.array(quaternion)))
        
        # Add odometry edge to previous pose
        if pose_id > 0:
            self.odometry_edges.append((pose_id - 1, pose_id))
        
        return pose_id
    
    def detect_loop_closures(self, point_clouds=None):
        """
        Detect loop closures based on distance and optionally point cloud similarity.
        
        Parameters:
            point_clouds: Optional list of downsampled point clouds for each pose
            
        Returns:
            List of loop closure constraints (pose1_id, pose2_id, confidence)
        """
        print("üîç –ü–æ–∏—Å–∫ –∑–∞–º—ã–∫–∞–Ω–∏–π —Ü–∏–∫–ª–æ–≤...")
        
        loop_closures = []
        num_poses = len(self.poses)
        
        if num_poses < 3:
            print("   ‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–æ–∑ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ü–∏–∫–ª–æ–≤")
            return loop_closures
        
        # Build KDTree for efficient spatial queries
        positions = np.array([pose[1] for pose in self.poses])
        kdtree = KDTree(positions)
        
        for i in range(num_poses):
            current_time = self.poses[i][0]
            current_pos = self.poses[i][1]
            
            # Find nearby poses
            nearby_indices = kdtree.query_ball_point(current_pos, self.loop_closure_distance)
            
            for j in nearby_indices:
                if j <= i:  # Only look at previous poses
                    continue
                    
                # Check time gap constraint
                time_gap = abs(current_time - self.poses[j][0])
                if time_gap < self.min_time_gap:
                    continue
                
                # Calculate confidence based on distance
                distance = np.linalg.norm(current_pos - self.poses[j][1])
                confidence = max(0, 1.0 - distance / self.loop_closure_distance)
                
                # Additional point cloud similarity check if available
                if point_clouds is not None and i < len(point_clouds) and j < len(point_clouds):
                    pc_similarity = self._calculate_point_cloud_similarity(
                        point_clouds[i], point_clouds[j]
                    )
                    confidence *= pc_similarity
                
                if confidence > 0.3:  # Threshold for accepting loop closure
                    loop_closures.append((i, j, confidence))
                    print(f"   ÔøΩ Loop closure: –ø–æ–∑–∞ {i} ‚Üî –ø–æ–∑–∞ {j} (confidence: {confidence:.3f})")
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(loop_closures)} –∑–∞–º—ã–∫–∞–Ω–∏–π —Ü–∏–∫–ª–æ–≤")
        self.loop_closure_edges = loop_closures
        return loop_closures
    
    def _calculate_point_cloud_similarity(self, pc1, pc2, max_points=1000):
        """
        Calculate similarity between two point clouds.
        
        Parameters:
            pc1, pc2: Point clouds as Nx3 arrays
            max_points: Maximum number of points to use for comparison
            
        Returns:
            Similarity score between 0 and 1
        """
        if pc1 is None or pc2 is None or len(pc1) == 0 or len(pc2) == 0:
            return 0.0
        
        # Downsample for efficiency
        if len(pc1) > max_points:
            indices = np.random.choice(len(pc1), max_points, replace=False)
            pc1 = pc1[indices]
        
        if len(pc2) > max_points:
            indices = np.random.choice(len(pc2), max_points, replace=False)
            pc2 = pc2[indices]
        
        try:
            # Simple geometric feature comparison
            # Calculate basic statistics for each point cloud
            stats1 = self._calculate_geometric_features(pc1)
            stats2 = self._calculate_geometric_features(pc2)
            
            # Compare features using normalized difference
            feature_diff = np.abs(stats1 - stats2) / (np.abs(stats1) + np.abs(stats2) + 1e-6)
            similarity = np.exp(-np.mean(feature_diff))
            
            return similarity
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –æ–±–ª–∞–∫–æ–≤: {e}")
            return 0.0
    
    def _calculate_geometric_features(self, pc):
        """Calculate basic geometric features of a point cloud."""
        if len(pc) == 0:
            return np.zeros(6)
        
        # Basic statistics
        mean_pos = np.mean(pc, axis=0)
        std_pos = np.std(pc, axis=0)
        
        return np.concatenate([mean_pos, std_pos])
    
    def optimize_poses(self, max_iterations=50):
        """
        Optimize the pose graph using least squares optimization.
        
        Returns:
            Optimized poses as list of (timestamp, position, quaternion)
        """
        print("üîß –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞ –ø–æ–∑...")
        
        if len(self.poses) < 2:
            print("   ‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–æ–∑ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏")
            return [pose for pose in self.poses]
        
        # Convert poses to optimization variables (x, y, z, qx, qy, qz, qw)
        initial_params = []
        for timestamp, position, quaternion in self.poses:
            initial_params.extend(position)  # x, y, z
            initial_params.extend(quaternion)  # qx, qy, qz, qw
        
        initial_params = np.array(initial_params)
        
        print(f"   üìä –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è {len(self.poses)} –ø–æ–∑ —Å {len(self.odometry_edges)} –æ–¥–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–º–∏ –∏ {len(self.loop_closure_edges)} loop closure —Å–≤—è–∑—è–º–∏")
        
        try:
            # Run optimization
            result = least_squares(
                self._pose_graph_residuals,
                initial_params,
                max_nfev=max_iterations * len(initial_params),
                verbose=0
            )
            
            if result.success:
                print(f"   ‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {result.nfev} –∏—Ç–µ—Ä–∞—Ü–∏–π")
                print(f"   üìà –°–Ω–∏–∂–µ–Ω–∏–µ –æ—à–∏–±–∫–∏: {result.fun[0]:.6f} ‚Üí {result.fun[-1]:.6f}")
            else:
                print(f"   ‚ö†Ô∏è  –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–µ —Å–æ—à–ª–∞—Å—å: {result.message}")
            
            # Extract optimized poses
            optimized_poses = []
            for i in range(len(self.poses)):
                start_idx = i * 7
                position = result.x[start_idx:start_idx+3]
                quaternion = result.x[start_idx+3:start_idx+7]
                
                # Normalize quaternion
                quaternion = quaternion / np.linalg.norm(quaternion)
                
                optimized_poses.append((
                    self.poses[i][0],  # Keep original timestamp
                    position,
                    quaternion
                ))
            
            return optimized_poses
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {e}")
            return [pose for pose in self.poses]
    
    def _pose_graph_residuals(self, params):
        """
        Calculate residuals for pose graph optimization.
        
        Parameters:
            params: Flattened array of pose parameters
            
        Returns:
            Array of residuals
        """
        residuals = []
        
        # Extract poses from parameters
        poses = []
        for i in range(len(self.poses)):
            start_idx = i * 7
            position = params[start_idx:start_idx+3]
            quaternion = params[start_idx+3:start_idx+7]
            poses.append((position, quaternion))
        
        # Odometry constraints
        for i, j in self.odometry_edges:
            if i < len(poses) and j < len(poses):
                # Calculate relative transformation
                rel_trans = poses[j][0] - poses[i][0]
                
                # Expected transformation based on original odometry
                expected_trans = self.poses[j][1] - self.poses[i][1]
                
                # Add residual for translation
                trans_residual = rel_trans - expected_trans
                residuals.extend(trans_residual)
                
                # Simple orientation constraint (could be improved)
                orient_residual = poses[j][1][:3] - poses[i][1][:3]  # Simplified
                residuals.extend(orient_residual * 0.1)  # Lower weight for orientation
        
        # Loop closure constraints
        for i, j, confidence in self.loop_closure_edges:
            if i < len(poses) and j < len(poses):
                # Loop closure should minimize distance between poses
                distance_residual = poses[i][0] - poses[j][0]
                residuals.extend(distance_residual * confidence)
        
        return np.array(residuals)
    
    def calculate_trajectory_metrics(self, original_poses, optimized_poses):
        """
        Calculate metrics to evaluate trajectory improvement.
        
        Returns:
            Dictionary with various metrics
        """
        if len(original_poses) != len(optimized_poses):
            return {}
        
        # Calculate total trajectory length before and after
        def trajectory_length(poses):
            length = 0.0
            for i in range(1, len(poses)):
                length += np.linalg.norm(poses[i][1] - poses[i-1][1])
            return length
        
        orig_length = trajectory_length(original_poses)
        opt_length = trajectory_length(optimized_poses)
        
        # Calculate drift (distance between start and end if it should be a loop)
        if len(self.loop_closure_edges) > 0:
            orig_drift = np.linalg.norm(original_poses[-1][1] - original_poses[0][1])
            opt_drift = np.linalg.norm(optimized_poses[-1][1] - optimized_poses[0][1])
        else:
            orig_drift = opt_drift = 0.0
        
        # Calculate average position change
        pos_changes = []
        for i in range(len(original_poses)):
            change = np.linalg.norm(optimized_poses[i][1] - original_poses[i][1])
            pos_changes.append(change)
        
        return {
            'original_length': orig_length,
            'optimized_length': opt_length,
            'original_drift': orig_drift,
            'optimized_drift': opt_drift,
            'average_position_change': np.mean(pos_changes),
            'max_position_change': np.max(pos_changes),
            'loop_closures_found': len(self.loop_closure_edges)
        }

def apply_slam_optimization(odom_timestamps, odom_positions, odom_orientations, 
                           point_clouds=None, enable_loop_closure=True):
    """
    Apply SLAM optimization to trajectory data.
    
    Parameters:
        odom_timestamps: Array of timestamps
        odom_positions: Nx3 array of positions
        odom_orientations: Nx4 array of quaternions
        point_clouds: Optional list of point clouds for loop closure detection
        enable_loop_closure: Whether to perform loop closure detection
        
    Returns:
        Tuple of (optimized_positions, optimized_orientations, metrics)
    """
    print("\nü§ñ –ü–†–ò–ú–ï–ù–ï–ù–ò–ï SLAM –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò...")
    print("-" * 50)
    
    # Initialize SLAM system
    slam = SimpleSLAM(
        loop_closure_distance=3.0,  # 3 meters for loop closure detection
        min_time_gap=15.0           # 15 seconds minimum gap
    )
    
    # Add all poses to the graph
    print(f"üìä –î–æ–±–∞–≤–ª–µ–Ω–∏–µ {len(odom_timestamps)} –ø–æ–∑ –≤ –≥—Ä–∞—Ñ...")
    for i in range(len(odom_timestamps)):
        slam.add_pose(odom_timestamps[i], odom_positions[i], odom_orientations[i])
    
    # Detect loop closures if enabled
    if enable_loop_closure:
        slam.detect_loop_closures(point_clouds)
    else:
        print("‚ö†Ô∏è  Loop closure detection –æ—Ç–∫–ª—é—á–µ–Ω")
    
    # Store original poses for comparison
    original_poses = [(odom_timestamps[i], odom_positions[i], odom_orientations[i]) 
                     for i in range(len(odom_timestamps))]
    
    # Optimize the pose graph
    optimized_poses = slam.optimize_poses()
    
    # Extract optimized data
    opt_positions = np.array([pose[1] for pose in optimized_poses])
    opt_orientations = np.array([pose[2] for pose in optimized_poses])
    
    # Calculate metrics
    metrics = slam.calculate_trajectory_metrics(original_poses, optimized_poses)
    
    # Print results
    print(f"\nÔøΩ –†–ï–ó–£–õ–¨–¢–ê–¢–´ SLAM –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò:")
    print(f"   ‚Ä¢ –ù–∞–π–¥–µ–Ω–æ loop closures: {metrics.get('loop_closures_found', 0)}")
    print(f"   ‚Ä¢ –ò—Å—Ö–æ–¥–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏: {metrics.get('original_length', 0):.2f} –º")
    print(f"   ‚Ä¢ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª–∏–Ω–∞: {metrics.get('optimized_length', 0):.2f} –º")
    if metrics.get('loop_closures_found', 0) > 0:
        print(f"   ‚Ä¢ –ò—Å—Ö–æ–¥–Ω—ã–π –¥—Ä–∏—Ñ—Ç: {metrics.get('original_drift', 0):.3f} –º")
        print(f"   ‚Ä¢ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥—Ä–∏—Ñ—Ç: {metrics.get('optimized_drift', 0):.3f} –º")
    print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–∏: {metrics.get('average_position_change', 0):.3f} –º")
    print(f"   ‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ: {metrics.get('max_position_change', 0):.3f} –º")
    
    return opt_positions, opt_orientations, metrics

def visualize_trajectory_comparison(original_positions, optimized_positions, slam_metrics, 
                                  output_dir, bag_filename):
    """
    Create visualization comparing original and optimized trajectories.
    
    Parameters:
        original_positions: Nx3 array of original positions
        optimized_positions: Nx3 array of optimized positions  
        slam_metrics: SLAM metrics dictionary
        output_dir: Directory to save plots
        bag_filename: Base filename for naming plots
    """
    try:
        print("üìä –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π...")
        
        # Create figure with subplots
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        # Plot 1: 2D trajectory comparison (XY plane)
        ax1.plot(original_positions[:, 0], original_positions[:, 1], 
                'r-', alpha=0.7, linewidth=2, label='–ò—Å—Ö–æ–¥–Ω–∞—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è')
        ax1.plot(optimized_positions[:, 0], optimized_positions[:, 1], 
                'b-', alpha=0.7, linewidth=2, label='–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è')
        ax1.scatter(original_positions[0, 0], original_positions[0, 1], 
                   c='green', s=100, marker='o', label='–°—Ç–∞—Ä—Ç', zorder=5)
        ax1.scatter(original_positions[-1, 0], original_positions[-1, 1], 
                   c='red', s=100, marker='s', label='–§–∏–Ω–∏—à (–∏—Å—Ö–æ–¥–Ω—ã–π)', zorder=5)
        ax1.scatter(optimized_positions[-1, 0], optimized_positions[-1, 1], 
                   c='blue', s=100, marker='s', label='–§–∏–Ω–∏—à (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)', zorder=5)
        ax1.set_xlabel('X (–º)')
        ax1.set_ylabel('Y (–º)')
        ax1.set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π (–≤–∏–¥ —Å–≤–µ—Ä—Ö—É)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_aspect('equal')
        
        # Plot 2: Height profile
        distances_orig = np.cumsum(np.concatenate([[0], np.sqrt(np.sum(np.diff(original_positions, axis=0)**2, axis=1))]))
        distances_opt = np.cumsum(np.concatenate([[0], np.sqrt(np.sum(np.diff(optimized_positions, axis=0)**2, axis=1))]))
        
        ax2.plot(distances_orig, original_positions[:, 2], 'r-', alpha=0.7, linewidth=2, label='–ò—Å—Ö–æ–¥–Ω–∞—è')
        ax2.plot(distances_opt, optimized_positions[:, 2], 'b-', alpha=0.7, linewidth=2, label='–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è')
        ax2.set_xlabel('–†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –ø–æ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ (–º)')
        ax2.set_ylabel('–í—ã—Å–æ—Ç–∞ Z (–º)')
        ax2.set_title('–ü—Ä–æ—Ñ–∏–ª—å –≤—ã—Å–æ—Ç—ã')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # Plot 3: Position corrections
        position_changes = np.sqrt(np.sum((optimized_positions - original_positions)**2, axis=1))
        ax3.plot(position_changes, 'g-', linewidth=2)
        ax3.set_xlabel('–ù–æ–º–µ—Ä –ø–æ–∑–∏—Ü–∏–∏')
        ax3.set_ylabel('–í–µ–ª–∏—á–∏–Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ (–º)')
        ax3.set_title('–ö–æ—Ä—Ä–µ–∫—Ü–∏–∏ –ø–æ–∑–∏—Ü–∏–π SLAM')
        ax3.grid(True, alpha=0.3)
        
        # Plot 4: Metrics summary
        ax4.axis('off')
        
        # Prepare metrics text
        metrics_text = f"""–†–ï–ó–£–õ–¨–¢–ê–¢–´ SLAM –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò:

‚Ä¢ –ù–∞–π–¥–µ–Ω–æ –∑–∞–º—ã–∫–∞–Ω–∏–π —Ü–∏–∫–ª–æ–≤: {slam_metrics.get('loop_closures_found', 0)}

‚Ä¢ –î–ª–∏–Ω–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏:
  - –ò—Å—Ö–æ–¥–Ω–∞—è: {slam_metrics.get('original_length', 0):.1f} –º
  - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è: {slam_metrics.get('optimized_length', 0):.1f} –º

‚Ä¢ –î—Ä–∏—Ñ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏:
  - –ò—Å—Ö–æ–¥–Ω—ã–π: {slam_metrics.get('original_drift', 0):.3f} –º
  - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π: {slam_metrics.get('optimized_drift', 0):.3f} –º

‚Ä¢ –ö–æ—Ä—Ä–µ–∫—Ü–∏–∏ –ø–æ–∑–∏—Ü–∏–π:
  - –°—Ä–µ–¥–Ω—è—è: {slam_metrics.get('average_position_change', 0):.3f} –º
  - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è: {slam_metrics.get('max_position_change', 0):.3f} –º

‚Ä¢ –£–ª—É—á—à–µ–Ω–∏—è:
  - –°–Ω–∏–∂–µ–Ω–∏–µ –¥—Ä–∏—Ñ—Ç–∞: {((slam_metrics.get('original_drift', 1) - slam_metrics.get('optimized_drift', 1)) / max(slam_metrics.get('original_drift', 1), 0.001) * 100):.1f}%
"""
        
        ax4.text(0.05, 0.95, metrics_text, transform=ax4.transAxes, fontsize=11,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        
        plt.tight_layout()
        
        # Save plot
        plot_filename = os.path.join(output_dir, f"{bag_filename}_slam_trajectory_comparison.png")
        plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"   ‚úÖ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {plot_filename}")
        
        return plot_filename
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
        return None

def choose_transform_mode():
    """
    Interactive function to choose transformation mode.
    
    Returns:
        tuple: (transform_mode, enable_slam) 
    """
    print(f"\nüéØ –í–´–ë–û–† –†–ï–ñ–ò–ú–ê –¢–†–ê–ù–°–§–û–†–ú–ê–¶–ò–ò:")
    print("=" * 60)
    print("1. üö´ –ë–µ–∑ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Å–µ–Ω—Å–æ—Ä–∞)")
    print("2. üåç –ì–ª–æ–±–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç (–æ–¥–æ–º–µ—Ç—Ä–∏—è)")
    print("3. üåç –ì–ª–æ–±–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ + SLAM –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)")
    print("4. üìç –õ–æ–∫–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–≤–æ–≥–æ —Å–∫–∞–Ω–∞)")
    print("5. ‚ùå –û—Ç–º–µ–Ω–∞")
    print()
    
    while True:
        try:
            choice = input("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ (1-5): ").strip()
            
            if choice == "1":
                print("‚úÖ –í—ã–±—Ä–∞–Ω —Ä–µ–∂–∏–º: –±–µ–∑ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏")
                return "none", False
            elif choice == "2":
                print("‚úÖ –í—ã–±—Ä–∞–Ω —Ä–µ–∂–∏–º: –≥–ª–æ–±–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç")
                return "global", False
            elif choice == "3":
                print("‚úÖ –í—ã–±—Ä–∞–Ω —Ä–µ–∂–∏–º: –≥–ª–æ–±–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ + SLAM –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è")
                return "global", True
            elif choice == "4":
                print("‚úÖ –í—ã–±—Ä–∞–Ω —Ä–µ–∂–∏–º: –ª–æ–∫–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç")
                return "local", False
            elif choice == "5":
                print("‚ùå –û—Ç–º–µ–Ω–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏")
                return "none", False
            else:
                print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä. –í–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–æ –æ—Ç 1 –¥–æ 5")
                
        except ValueError:
            print("‚ùå –û—à–∏–±–∫–∞ –≤–≤–æ–¥–∞. –í–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–æ")
        except KeyboardInterrupt:
            print("\nüëã –í—ã–±–æ—Ä –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            return "none", False

def get_pointcloud2_topics(bag_file):
    """
    Function to find all PointCloud2 topics from a given bag file.

    Parameters:
        bag_file (str): Path to the ROS bag file.

    Returns:
        dict: Dictionary with topic names as keys and message counts as values
    """
    print("üîç Scanning bag file for PointCloud2 topics...")
    # Dictionary to store occurrences of topics with PointCloud2 type
    pointcloud2_topics = defaultdict(int)
    
    with rosbag.Bag(bag_file, 'r') as bag:
        # Get total message count for progress
        try:
            total_msgs = bag.get_message_count()
            print(f"   üìä Analyzing {total_msgs:,} messages...")
        except:
            total_msgs = 0
            print("   üìä Analyzing messages...")
        
        msg_count = 0
        last_progress = -1
        
        for topic, msg, t in bag.read_messages():
            msg_count += 1
            if msg._type == 'sensor_msgs/PointCloud2':
                pointcloud2_topics[topic] += 1
            
            # Show progress every 5%
            if total_msgs > 0:
                progress = int((msg_count / total_msgs) * 100)
                if progress >= last_progress + 5:
                    print(f"   üìà Scan progress: {progress}% ({msg_count:,}/{total_msgs:,})")
                    last_progress = progress
    
    if pointcloud2_topics:
        print(f"   ‚úÖ Found PointCloud2 topics: {dict(pointcloud2_topics)}")
        return dict(pointcloud2_topics)
    else:
        print("   ‚ùå No PointCloud2 topics found")
        return {}

def choose_pointcloud2_topic(bag_file):
    """
    Interactive function to choose PointCloud2 topic from available options.
    
    Parameters:
        bag_file (str): Path to the ROS bag file.
        
    Returns:
        str: Selected topic name, or None if no topics available or user cancelled
    """
    # Get all available PointCloud2 topics
    pointcloud2_topics = get_pointcloud2_topics(bag_file)
    
    if not pointcloud2_topics:
        print("‚ùå ERROR: No PointCloud2 topics found in the bag file")
        return None
    
    # If only one topic, use it automatically
    if len(pointcloud2_topics) == 1:
        topic_name = list(pointcloud2_topics.keys())[0]
        message_count = pointcloud2_topics[topic_name]
        print(f"‚úÖ Only one PointCloud2 topic found: {topic_name} ({message_count:,} messages)")
        print("   üîÑ Using automatically...")
        return topic_name
    
    # Multiple topics - let user choose
    print(f"\nüéØ –í–´–ë–û–† –¢–û–ü–ò–ö–ê PointCloud2:")
    print("=" * 60)
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(pointcloud2_topics)} —Ç–æ–ø–∏–∫–æ–≤ PointCloud2:")
    print()
    
    topics_list = list(pointcloud2_topics.items())
    
    # Display topics with statistics
    for i, (topic, count) in enumerate(topics_list, 1):
        # Get additional topic info
        try:
            with rosbag.Bag(bag_file, 'r') as bag:
                bag_info = bag.get_type_and_topic_info()
                if topic in bag_info.topics:
                    topic_info = bag_info.topics[topic]
                    frequency = topic_info.frequency
                    freq_str = f"~{frequency:.1f} Hz" if frequency is not None and frequency > 0 else "unknown Hz"
                else:
                    freq_str = "unknown Hz"
        except:
            freq_str = "unknown Hz"
        
        print(f"   {i}. üì° {topic}")
        print(f"      üìä –°–æ–æ–±—â–µ–Ω–∏–π: {count:,}")
        print(f"      üîÑ –ß–∞—Å—Ç–æ—Ç–∞: {freq_str}")
        print()
    
    print("   0. ‚ùå –û—Ç–º–µ–Ω–∞")
    print()
    
    while True:
        try:
            choice = input(f"–í—ã–±–µ—Ä–∏—Ç–µ —Ç–æ–ø–∏–∫ (0-{len(topics_list)}): ").strip()
            
            if choice == "0":
                print("‚ùå –í—ã–±–æ—Ä —Ç–æ–ø–∏–∫–∞ –æ—Ç–º–µ–Ω–µ–Ω")
                return None
            
            choice_num = int(choice)
            if 1 <= choice_num <= len(topics_list):
                selected_topic = topics_list[choice_num - 1][0]
                selected_count = topics_list[choice_num - 1][1]
                print(f"\n‚úÖ –í—ã–±—Ä–∞–Ω —Ç–æ–ø–∏–∫: {selected_topic}")
                print(f"   üìä –°–æ–æ–±—â–µ–Ω–∏–π –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {selected_count:,}")
                
                # Confirm choice
                confirm = input(f"‚ùì –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å —ç—Ç–∏–º —Ç–æ–ø–∏–∫–æ–º? (y/n): ").strip().lower()
                if confirm in ['y', 'yes', '–¥', '–¥–∞']:
                    return selected_topic
                else:
                    print("üîÑ –í—ã–±–µ—Ä–∏—Ç–µ –¥—Ä—É–≥–æ–π —Ç–æ–ø–∏–∫...")
                    continue
            else:
                print(f"‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä. –í–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–æ –æ—Ç 0 –¥–æ {len(topics_list)}")
                
        except ValueError:
            print("‚ùå –û—à–∏–±–∫–∞ –≤–≤–æ–¥–∞. –í–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–æ")
        except KeyboardInterrupt:
            print("\nüëã –í—ã–±–æ—Ä –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            return None

def get_topic_detailed_info(bag_file, topic_name):
    """
    Get detailed information about a specific topic.
    
    Parameters:
        bag_file (str): Path to the ROS bag file
        topic_name (str): Name of the topic to analyze
        
    Returns:
        dict: Detailed topic information
    """
    try:
        with rosbag.Bag(bag_file, 'r') as bag:
            bag_info = bag.get_type_and_topic_info()
            
            if topic_name not in bag_info.topics:
                return None
                
            topic_info = bag_info.topics[topic_name]
            
            # Get time range for this topic
            first_time = None
            last_time = None
            
            for topic, msg, t in bag.read_messages(topics=[topic_name]):
                if first_time is None:
                    first_time = t.to_sec()
                last_time = t.to_sec()
            
            duration = last_time - first_time if (first_time and last_time) else 0
            
            return {
                'message_count': topic_info.message_count,
                'msg_type': topic_info.msg_type,
                'frequency': topic_info.frequency,
                'first_time': first_time,
                'last_time': last_time,
                'duration': duration
            }
    except Exception as e:
        print(f"‚ö†Ô∏è  Warning: Could not get detailed info for topic {topic_name}: {e}")
        return None

def get_odometry_topics(bag_file):
    """
    Function to find all Odometry topics from a given bag file.

    Parameters:
        bag_file (str): Path to the ROS bag file.

    Returns:
        dict: Dictionary with topic names as keys and message counts as values
    """
    print("üîç Scanning bag file for Odometry topics...")
    odometry_topics = defaultdict(int)
    
    with rosbag.Bag(bag_file, 'r') as bag:
        try:
            total_msgs = bag.get_message_count()
            print(f"   üìä Analyzing {total_msgs:,} messages for odometry...")
        except:
            total_msgs = 0
            print("   üìä Analyzing messages for odometry...")
        
        msg_count = 0
        last_progress = -1
        
        for topic, msg, t in bag.read_messages():
            msg_count += 1
            if msg._type == 'nav_msgs/Odometry':
                odometry_topics[topic] += 1
            
            # Show progress every 10%
            if total_msgs > 0:
                progress = int((msg_count / total_msgs) * 100)
                if progress >= last_progress + 10:
                    print(f"   üìà Odometry scan progress: {progress}% ({msg_count:,}/{total_msgs:,})")
                    last_progress = progress
    
    if odometry_topics:
        print(f"   ‚úÖ Found Odometry topics: {dict(odometry_topics)}")
        return dict(odometry_topics)
    else:
        print("   ‚ùå No Odometry topics found")
        return {}

def choose_odometry_topic(bag_file):
    """
    Interactive function to choose Odometry topic from available options.
    
    Parameters:
        bag_file (str): Path to the ROS bag file.
        
    Returns:
        str: Selected topic name, or None if no topics available or user cancelled
    """
    # Get all available Odometry topics
    odometry_topics = get_odometry_topics(bag_file)
    
    if not odometry_topics:
        print("‚ùå No Odometry topics found in the bag file")
        return None
    
    # If only one topic, use it automatically
    if len(odometry_topics) == 1:
        topic_name = list(odometry_topics.keys())[0]
        message_count = odometry_topics[topic_name]
        print(f"‚úÖ Only one Odometry topic found: {topic_name} ({message_count:,} messages)")
        print("   üîÑ Using automatically for .POS file generation...")
        return topic_name
    
    # Multiple topics - let user choose
    print(f"\nüéØ –í–´–ë–û–† –¢–û–ü–ò–ö–ê ODOMETRY:")
    print("=" * 60)
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(odometry_topics)} —Ç–æ–ø–∏–∫–æ–≤ Odometry:")
    print()
    
    topics_list = list(odometry_topics.items())
    
    # Display topics with statistics
    for i, (topic, count) in enumerate(topics_list, 1):
        try:
            with rosbag.Bag(bag_file, 'r') as bag:
                bag_info = bag.get_type_and_topic_info()
                if topic in bag_info.topics:
                    topic_info = bag_info.topics[topic]
                    frequency = topic_info.frequency
                    freq_str = f"~{frequency:.1f} Hz" if frequency is not None and frequency > 0 else "unknown Hz"
                else:
                    freq_str = "unknown Hz"
        except:
            freq_str = "unknown Hz"
        
        print(f"   {i}. üß≠ {topic}")
        print(f"      üìä –°–æ–æ–±—â–µ–Ω–∏–π: {count:,}")
        print(f"      üîÑ –ß–∞—Å—Ç–æ—Ç–∞: {freq_str}")
        print()
    
    print("   0. ‚ùå –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Å–æ–∑–¥–∞–Ω–∏–µ .POS —Ñ–∞–π–ª–∞")
    print()
    
    while True:
        try:
            choice = input(f"–í—ã–±–µ—Ä–∏—Ç–µ —Ç–æ–ø–∏–∫ –æ–¥–æ–º–µ—Ç—Ä–∏–∏ (0-{len(topics_list)}): ").strip()
            
            if choice == "0":
                print("‚ùå –°–æ–∑–¥–∞–Ω–∏–µ .POS —Ñ–∞–π–ª–∞ –ø—Ä–æ–ø—É—â–µ–Ω–æ")
                return None
            
            choice_num = int(choice)
            if 1 <= choice_num <= len(topics_list):
                selected_topic = topics_list[choice_num - 1][0]
                selected_count = topics_list[choice_num - 1][1]
                print(f"\n‚úÖ –í—ã–±—Ä–∞–Ω —Ç–æ–ø–∏–∫ –æ–¥–æ–º–µ—Ç—Ä–∏–∏: {selected_topic}")
                print(f"   üìä –°–æ–æ–±—â–µ–Ω–∏–π –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {selected_count:,}")
                return selected_topic
            else:
                print(f"‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä. –í–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–æ –æ—Ç 0 –¥–æ {len(topics_list)}")
                
        except ValueError:
            print("‚ùå –û—à–∏–±–∫–∞ –≤–≤–æ–¥–∞. –í–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–æ")
        except KeyboardInterrupt:
            print("\nüëã –í—ã–±–æ—Ä –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            return None

def create_pos_file(bag_file, output_dir, odometry_topic):
    """
    Create a .POS file from odometry data in the bag file.
    
    Parameters:
        bag_file (str): Path to the ROS bag file
        output_dir (str): Directory to save the .POS file
        odometry_topic (str): Name of the odometry topic to extract
    """
    if not odometry_topic:
        print("‚ö†Ô∏è  No odometry topic provided, skipping .POS file creation")
        return
    
    try:
        print(f"\nüß≠ –°–û–ó–î–ê–ù–ò–ï .POS –§–ê–ô–õ–ê...")
        print("-" * 50)
        print(f"üì° –ò—Å—Ç–æ—á–Ω–∏–∫: —Ç–æ–ø–∏–∫ '{odometry_topic}'")
        
        # Extract base filename
        base_filename = os.path.splitext(os.path.basename(bag_file))[0]
        pos_file = os.path.join(output_dir, base_filename + ".pos")
        print(f"üéØ –í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {pos_file}")
        
        positions = []
        orientations = []
        timestamps = []
        
        print("üîÑ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ–¥–æ–º–µ—Ç—Ä–∏–∏...")
        
        with rosbag.Bag(bag_file, 'r') as bag:
            # Get total message count for this topic
            total_messages = bag.get_message_count(topic_filters=odometry_topic)
            print(f"   üìä –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π –æ–¥–æ–º–µ—Ç—Ä–∏–∏: {total_messages:,}")
            
            if total_messages == 0:
                print("‚ùå –ù–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –æ–¥–æ–º–µ—Ç—Ä–∏–∏ –≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–º —Ç–æ–ø–∏–∫–µ")
                return
            
            message_count = 0
            start_time = time.time()
            
            for topic, msg, ros_timestamp in bag.read_messages(topics=[odometry_topic]):
                message_count += 1
                
                # Extract position
                pos = msg.pose.pose.position
                positions.append([pos.x, pos.y, pos.z])
                
                # Extract orientation (quaternion)
                orient = msg.pose.pose.orientation
                orientations.append([orient.x, orient.y, orient.z, orient.w])
                
                # Use ROS timestamp
                timestamps.append(ros_timestamp.to_sec())
                
                # Progress indicator
                if message_count % max(1, total_messages // 20) == 0 or message_count <= 5:
                    progress = (message_count / total_messages) * 100
                    print(f"   üìä [{progress:5.1f}%] –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {message_count:,}/{total_messages:,} —Å–æ–æ–±—â–µ–Ω–∏–π")
        
        elapsed_time = time.time() - start_time
        print(f"   ‚è±Ô∏è  –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥")
        
        # Convert to numpy arrays
        positions = np.array(positions)
        orientations = np.array(orientations)
        timestamps = np.array(timestamps)
        
        print(f"\nüìä –ê–ù–ê–õ–ò–ó –¢–†–ê–ï–ö–¢–û–†–ò–ò:")
        print(f"   ‚Ä¢ –¢–æ—á–µ–∫ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏: {len(positions):,}")
        print(f"   ‚Ä¢ –í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω: {timestamps[-1] - timestamps[0]:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è —á–∞—Å—Ç–æ—Ç–∞: {len(positions) / (timestamps[-1] - timestamps[0]):.1f} Hz")
        
        # Calculate trajectory statistics
        if len(positions) > 1:
            distances = np.sqrt(np.sum(np.diff(positions, axis=0)**2, axis=1))
            total_distance = np.sum(distances)
            max_distance_step = np.max(distances)
            avg_distance_step = np.mean(distances)
            
            print(f"   ‚Ä¢ –û–±—â–∞—è –¥–∏—Å—Ç–∞–Ω—Ü–∏—è: {total_distance:.2f} –º")
            print(f"   ‚Ä¢ –ú–∞–∫—Å. —à–∞–≥: {max_distance_step:.3f} –º")
            print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π —à–∞–≥: {avg_distance_step:.3f} –º")
        
        # Position ranges
        print(f"   ‚Ä¢ –î–∏–∞–ø–∞–∑–æ–Ω X: {np.min(positions[:, 0]):.6f} –¥–æ {np.max(positions[:, 0]):.6f}")
        print(f"   ‚Ä¢ –î–∏–∞–ø–∞–∑–æ–Ω Y: {np.min(positions[:, 1]):.6f} –¥–æ {np.max(positions[:, 1]):.6f}")
        print(f"   ‚Ä¢ –î–∏–∞–ø–∞–∑–æ–Ω Z: {np.min(positions[:, 2]):.6f} –¥–æ {np.max(positions[:, 2]):.6f}")
        
        # Convert quaternions to Euler angles (roll, pitch, yaw)
        def quaternion_to_euler(q):
            """Convert quaternion to euler angles (roll, pitch, yaw) in degrees"""
            x, y, z, w = q
            
            # Roll (x-axis rotation)
            sinr_cosp = 2 * (w * x + y * z)
            cosr_cosp = 1 - 2 * (x * x + y * y)
            roll = np.arctan2(sinr_cosp, cosr_cosp)
            
            # Pitch (y-axis rotation)
            sinp = 2 * (w * y - z * x)
            if np.abs(sinp) >= 1:
                pitch = np.copysign(np.pi / 2, sinp)  # use 90 degrees if out of range
            else:
                pitch = np.arcsin(sinp)
            
            # Yaw (z-axis rotation)
            siny_cosp = 2 * (w * z + x * y)
            cosy_cosp = 1 - 2 * (y * y + z * z)
            yaw = np.arctan2(siny_cosp, cosy_cosp)
            
            return np.degrees([roll, pitch, yaw])
        
        print("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∫–≤–∞—Ç–µ—Ä–Ω–∏–æ–Ω–æ–≤ –≤ —É–≥–ª—ã –≠–π–ª–µ—Ä–∞...")
        euler_angles = np.array([quaternion_to_euler(q) for q in orientations])
        
        print(f"   ‚Ä¢ –î–∏–∞–ø–∞–∑–æ–Ω Roll: {np.min(euler_angles[:, 0]):.2f}¬∞ –¥–æ {np.max(euler_angles[:, 0]):.2f}¬∞")
        print(f"   ‚Ä¢ –î–∏–∞–ø–∞–∑–æ–Ω Pitch: {np.min(euler_angles[:, 1]):.2f}¬∞ –¥–æ {np.max(euler_angles[:, 1]):.2f}¬∞")
        print(f"   ‚Ä¢ –î–∏–∞–ø–∞–∑–æ–Ω Yaw: {np.min(euler_angles[:, 2]):.2f}¬∞ –¥–æ {np.max(euler_angles[:, 2]):.2f}¬∞")
        
        # Write .POS file
        print(f"\nüíæ –ó–ê–ü–ò–°–¨ .POS –§–ê–ô–õ–ê...")
        
        with open(pos_file, 'w') as f:
            # Write header
            f.write("% POS file generated from ROS bag odometry data\n")
            f.write(f"% Source bag: {os.path.basename(bag_file)}\n")
            f.write(f"% Odometry topic: {odometry_topic}\n")
            f.write(f"% Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("% Format: GPS_Time X Y Z Roll Pitch Yaw\n")
            f.write("% Units: seconds, meters, degrees\n")
            f.write("%\n")
            
            # Write data
            for i in range(len(positions)):
                gps_time = timestamps[i]
                x, y, z = positions[i]
                roll, pitch, yaw = euler_angles[i]
                
                f.write(f"{gps_time:.6f} {x:.6f} {y:.6f} {z:.6f} {roll:.6f} {pitch:.6f} {yaw:.6f}\n")
        
        # Verify file creation
        if os.path.exists(pos_file):
            file_size = os.path.getsize(pos_file)
            print(f"‚úÖ .POS —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ!")
            print(f"   üìÅ –§–∞–π–ª: {pos_file}")
            print(f"   üìä –†–∞–∑–º–µ—Ä: {file_size:,} –±–∞–π—Ç")
            print(f"   üî¢ –ó–∞–ø–∏—Å–µ–π: {len(positions):,}")
            
            # Verify by reading first and last few lines
            with open(pos_file, 'r') as f:
                lines = f.readlines()
                data_lines = [line for line in lines if not line.startswith('%')]
                print(f"   ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞: {len(data_lines)} —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö")
                
                if len(data_lines) >= 3:
                    print(f"   üîç –ü–µ—Ä–≤–∞—è –∑–∞–ø–∏—Å—å: {data_lines[0].strip()}")
                    print(f"   üîç –ü–æ—Å–ª–µ–¥–Ω—è—è –∑–∞–ø–∏—Å—å: {data_lines[-1].strip()}")
        else:
            print("‚ùå –û—à–∏–±–∫–∞: .POS —Ñ–∞–π–ª –Ω–µ –±—ã–ª —Å–æ–∑–¥–∞–Ω")
    
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è .POS —Ñ–∞–π–ª–∞: {e}")
        import traceback
        traceback.print_exc()

def choose_express_mode():
    """
    Ask user if they want to use express mode with default parameters.
    
    Returns:
        bool: True if express mode selected
    """
    print(f"\n‚ö° –≠–ö–°–ü–†–ï–°–°-–†–ï–ñ–ò–ú:")
    print("=" * 60)
    print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã?")
    print("  ‚Ä¢ –¢–æ–ø–∏–∫ –æ–±–ª–∞–∫–∞ —Ç–æ—á–µ–∫: /cloud_registered")
    print("  ‚Ä¢ –¢–æ–ø–∏–∫ –æ–¥–æ–º–µ—Ç—Ä–∏–∏: /lio/odom")
    print("  ‚Ä¢ –†–µ–∂–∏–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏: –ë–µ–∑ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏")
    print()
    print("1. ‚ö° –î–∞, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç–∫—Å–ø—Ä–µ—Å—Å-—Ä–µ–∂–∏–º")
    print("2. ‚öôÔ∏è  –ù–µ—Ç, –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –≤—Ä—É—á–Ω—É—é")
    print()
    
    while True:
        try:
            choice = input("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º (1-2): ").strip()
            
            if choice == "1":
                print("‚úÖ –í—ã–±—Ä–∞–Ω —ç–∫—Å–ø—Ä–µ—Å—Å-—Ä–µ–∂–∏–º")
                return True
            elif choice == "2":
                print("‚úÖ –í—ã–±—Ä–∞–Ω–∞ —Ä—É—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞")
                return False
            else:
                print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä. –í–≤–µ–¥–∏—Ç–µ 1 –∏–ª–∏ 2")
                
        except KeyboardInterrupt:
            print("\nüëã –í—ã–±–æ—Ä –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            return False

def convert_bag_to_laz(bag_file, output_dir, selected_topic=None, transform_mode=None, enable_slam=None, express_mode=False):
    """
    Function to convert PointCloud2 data from a ROS bag file to a LAZ file with optional transformation and SLAM optimization.

    Parameters:
        bag_file (str): Path to the ROS bag file.
        output_dir (str): Directory to save the generated LAZ files.
        selected_topic (str): Specific topic to process (if None, will prompt user to choose)
        transform_mode (str): Transformation mode ('none', 'global', 'local', None for user choice)
        enable_slam (bool): Whether to enable SLAM optimization (None for user choice)
        express_mode (bool): Use express mode with default parameters
    """
    try:
        print("="*80)
        print(f"STARTING CONVERSION: {os.path.basename(bag_file)}")
        print("="*80)
        
        # First, analyze all topics in the bag file
        bag_analysis = analyze_bag_topics(bag_file)
        
        if not bag_analysis:
            print("‚ùå ERROR: Could not analyze bag file")
            return
        
        # Ask about express mode if not already set
        if not express_mode and selected_topic is None:
            express_mode = choose_express_mode()
        
        # Apply express mode defaults
        if express_mode:
            print("\n‚ö° –ü–†–ò–ú–ï–ù–ï–ù–ò–ï –≠–ö–°–ü–†–ï–°–°-–†–ï–ñ–ò–ú–ê...")
            
            # Check if default topics exist
            available_pc_topics = get_pointcloud2_topics(bag_file)
            available_odom_topics = get_odometry_topics(bag_file)
            
            # Set default pointcloud topic
            if selected_topic is None:
                if "/cloud_registered" in available_pc_topics:
                    selected_topic = "/cloud_registered"
                    print(f"‚úÖ –¢–æ–ø–∏–∫ –æ–±–ª–∞–∫–∞ —Ç–æ—á–µ–∫: {selected_topic}")
                else:
                    print(f"‚ö†Ô∏è  –¢–æ–ø–∏–∫ /cloud_registered –Ω–µ –Ω–∞–π–¥–µ–Ω")
                    print(f"   –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–æ–ø–∏–∫–∏: {list(available_pc_topics.keys())}")
                    selected_topic = choose_pointcloud2_topic(bag_file)
            
            # Set default odometry topic
            if "/lio/odom" in available_odom_topics:
                odometry_topic = "/lio/odom"
                print(f"‚úÖ –¢–æ–ø–∏–∫ –æ–¥–æ–º–µ—Ç—Ä–∏–∏: {odometry_topic}")
            else:
                print(f"‚ö†Ô∏è  –¢–æ–ø–∏–∫ /lio/odom –Ω–µ –Ω–∞–π–¥–µ–Ω")
                print(f"   –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–æ–ø–∏–∫–∏: {list(available_odom_topics.keys())}")
                odometry_topic = choose_odometry_topic(bag_file)
            
            # Set default transformation mode
            if transform_mode is None:
                transform_mode = "none"
                enable_slam = False
                print(f"‚úÖ –†–µ–∂–∏–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏: –±–µ–∑ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏")
        else:
            # Wait for user to review the analysis
            input("\nüìã –ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
            odometry_topic = None
        
        # Choose the PointCloud2 topic (if not set by express mode)
        if selected_topic is None:
            pointcloud2_topic = choose_pointcloud2_topic(bag_file)
            if not pointcloud2_topic:
                print("‚ùå ERROR: No topic selected or no PointCloud2 topics available")
                return
        else:
            # Verify the specified topic exists and is PointCloud2
            available_topics = get_pointcloud2_topics(bag_file)
            if selected_topic not in available_topics:
                print(f"‚ùå ERROR: Topic '{selected_topic}' not found in available PointCloud2 topics")
                print(f"Available topics: {list(available_topics.keys())}")
                return
            pointcloud2_topic = selected_topic
        
        print(f"‚úÖ Processing topic: {pointcloud2_topic}")
        
        # Choose odometry topic for .POS file and transformation (if not set by express mode)
        if not express_mode:
            print(f"\nüß≠ –ü–û–ò–°–ö –¢–û–ü–ò–ö–û–í –û–î–û–ú–ï–¢–†–ò–ò...")
            odometry_topic = choose_odometry_topic(bag_file)
        
        # Choose transformation mode (if not set by express mode)
        if transform_mode is None or enable_slam is None:
            if odometry_topic:
                transform_mode, enable_slam = choose_transform_mode()
            else:
                print("‚ö†Ô∏è  –ù–µ—Ç —Ç–æ–ø–∏–∫–æ–≤ –æ–¥–æ–º–µ—Ç—Ä–∏–∏ - —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
                transform_mode = "none"
                enable_slam = False
        
        print(f"üîß –†–µ–∂–∏–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏: {transform_mode}")
        if enable_slam:
            print("ü§ñ SLAM –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –≤–∫–ª—é—á–µ–Ω–∞")
        
        # Load odometry data if transformation is needed
        odom_timestamps = None
        odom_positions = None 
        odom_orientations = None
        slam_metrics = None
        
        if transform_mode in ["global", "local"] and odometry_topic:
            print(f"\nüîÑ –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –û–î–û–ú–ï–¢–†–ò–ò...")
            odom_timestamps, odom_positions, odom_orientations = collect_odometry_data(bag_file, odometry_topic)
            
            if odom_timestamps is None:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ–¥–æ–º–µ—Ç—Ä–∏–∏, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–µ–∂–∏–º –±–µ–∑ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏")
                transform_mode = "none"
                enable_slam = False
            elif enable_slam and transform_mode == "global":
                # Save original positions for visualization
                odom_positions_original = odom_positions.copy()
                odom_orientations_original = odom_orientations.copy()
                
                # Apply SLAM optimization
                odom_positions, odom_orientations, slam_metrics = apply_slam_optimization(
                    odom_timestamps, odom_positions, odom_orientations, 
                    point_clouds=None,  # Will be populated during processing if needed
                    enable_loop_closure=True
                )
        
        # Get detailed topic information
        topic_details = get_topic_detailed_info(bag_file, pointcloud2_topic)
        if topic_details:
            print(f"üìä Topic details:")
            print(f"   ‚Ä¢ Messages: {topic_details['message_count']:,}")
            print(f"   ‚Ä¢ Frequency: ~{topic_details['frequency']:.2f} Hz")
            if topic_details['duration'] > 0:
                print(f"   ‚Ä¢ Duration: {topic_details['duration']:.2f} seconds")
                print(f"   ‚Ä¢ Time range: {topic_details['first_time']:.3f} - {topic_details['last_time']:.3f}")
        
        # Open the ROS bag file
        print(f"üìÇ Opening ROS bag file: {bag_file}")
        bag = rosbag.Bag(bag_file, 'r')

        # Get bag info with progress
        print("üìä Analyzing bag file structure...")
        bag_info = bag.get_type_and_topic_info()
        print(f"   ‚Ä¢ Total topics: {len(bag_info.topics)}")
        if pointcloud2_topic in bag_info.topics:
            topic_info = bag_info.topics[pointcloud2_topic]
            print(f"   ‚Ä¢ PointCloud2 messages: {topic_info.message_count}")
            print(f"   ‚Ä¢ Message type: {topic_info.msg_type}")
            freq_display = f"~{topic_info.frequency:.2f} Hz" if topic_info.frequency is not None else "unknown Hz"
            print(f"   ‚Ä¢ Frequency: {freq_display}")

        # Get the total number of messages in the bag file
        print("üìà Counting PointCloud2 messages...")
        total_messages = bag.get_message_count(topic_filters=pointcloud2_topic)
        print(f"   ‚úÖ Total messages to process: {total_messages}")

        # Create empty lists to store coordinates and additional fields
        x_list = []
        y_list = []
        z_list = []
        intensity_list = []
        gps_time_list = []

        # Initialize counters and statistics
        message_count = 0
        total_points = 0
        points_per_message = []

        print("\nüîç ANALYZING MESSAGE STRUCTURE...")
        print("-" * 50)
        
        # Check available fields in the first message
        print("üîé Reading first message for field analysis...")
        first_msg = None
        first_points_sample = []
        
        for _, msg, _ in bag.read_messages(topics=[pointcloud2_topic]):
            first_msg = msg
            print("   ‚úÖ First message loaded")
            
            # Try to read a few points to check data format
            try:
                point_count = 0
                for point in pc2.read_points(msg, skip_nans=False):  # Don't skip NaNs initially
                    first_points_sample.append(point)
                    point_count += 1
                    if point_count >= 10:  # Read first 10 points
                        break
                print(f"   ‚úÖ Successfully read {len(first_points_sample)} sample points")
                
                # Check for NaN patterns
                if first_points_sample:
                    first_point = first_points_sample[0]
                    print(f"   üîç First point structure: {len(first_point)} fields")
                    print(f"   üîç First point values: {first_point}")
                    
                    # Check for NaN values in sample
                    nan_count = 0
                    for point in first_points_sample:
                        if any(np.isnan(val) if isinstance(val, float) else False for val in point[:3]):
                            nan_count += 1
                    print(f"   üìä NaN points in sample: {nan_count}/{len(first_points_sample)}")
                    
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Error reading sample points: {e}")
                
            break
        
        if not first_msg:
            print("‚ùå ERROR: Could not read first message")
            return

        available_fields = [field.name for field in first_msg.fields] if first_msg else []
        print(f"üìã Available fields ({len(available_fields)}): {available_fields}")
        
        # Add OS and library version info
        import platform
        print(f"\nüñ•Ô∏è  SYSTEM INFO:")
        print(f"   ‚Ä¢ OS: {platform.system()} {platform.release()}")
        print(f"   ‚Ä¢ Python: {platform.python_version()}")
        try:
            import sensor_msgs
            print(f"   ‚Ä¢ sensor_msgs version: {sensor_msgs.__version__ if hasattr(sensor_msgs, '__version__') else 'unknown'}")
        except:
            print(f"   ‚Ä¢ sensor_msgs version: unknown")
        print(f"   ‚Ä¢ numpy version: {np.__version__}")
        try:
            print(f"   ‚Ä¢ laspy version: {laspy.__version__}")
        except:
            print(f"   ‚Ä¢ laspy version: unknown")
        
        # Print detailed field information for debugging
        if first_msg:
            print("\nüìä Detailed field information:")
            for i, field in enumerate(first_msg.fields):
                datatype_names = {1: 'INT8', 2: 'UINT8', 3: 'INT16', 4: 'UINT16', 
                                5: 'INT32', 6: 'UINT32', 7: 'FLOAT32', 8: 'FLOAT64'}
                datatype_name = datatype_names.get(field.datatype, f'UNKNOWN({field.datatype})')
                print(f"   {i+1:2d}. {field.name:15s} | Type: {datatype_name:8s} | Offset: {field.offset:3d} | Count: {field.count}")
            
            print(f"\nüìê PointCloud2 dimensions:")
            print(f"   ‚Ä¢ Width: {first_msg.width}")
            print(f"   ‚Ä¢ Height: {first_msg.height}")
            print(f"   ‚Ä¢ Points per message: {first_msg.width * first_msg.height}")
            print(f"   ‚Ä¢ Point step: {first_msg.point_step} bytes")
            print(f"   ‚Ä¢ Row step: {first_msg.row_step} bytes")
            print(f"   ‚Ä¢ Is dense: {first_msg.is_dense}")
            
            # Check if message has header with timestamp
            if hasattr(first_msg, 'header') and hasattr(first_msg.header, 'stamp'):
                timestamp = first_msg.header.stamp.to_sec()
                print(f"   ‚Ä¢ Header timestamp: {timestamp:.6f} ({timestamp})")
                print(f"   ‚Ä¢ Frame ID: '{first_msg.header.frame_id}'")
                print("‚úÖ Will use ROS header timestamp as GPS time")
        
        # Determine which fields to extract based on availability
        fields_to_extract = ["x", "y", "z"]
        has_intensity = "intensity" in available_fields
        
        # Expanded search for time fields in PointCloud2 data
        possible_time_fields = ["gps_time", "time", "t", "timestamp", "time_stamp", "stamp"]
        has_pointcloud_time = False
        pointcloud_time_field = None
        
        for field_name in possible_time_fields:
            if field_name in available_fields:
                has_pointcloud_time = True
                pointcloud_time_field = field_name
                print(f"‚è∞ PointCloud2 time field found: {pointcloud_time_field}")
                break
        
        # Use ROS header timestamp if no time field in PointCloud2
        use_ros_time = not has_pointcloud_time and hasattr(first_msg, 'header')
        has_gps_time = has_pointcloud_time or use_ros_time
        
        print(f"\nüéØ FIELD EXTRACTION PLAN:")
        print(f"   ‚Ä¢ Coordinates (x,y,z): ‚úÖ Always included")
        
        if has_intensity:
            fields_to_extract.append("intensity")
            print(f"   ‚Ä¢ Intensity: ‚úÖ Found in PointCloud2")
        else:
            print(f"   ‚Ä¢ Intensity: ‚ùå Not found")
            
        if has_pointcloud_time:
            fields_to_extract.append(pointcloud_time_field)
            print(f"   ‚Ä¢ Time: ‚úÖ From PointCloud2 field '{pointcloud_time_field}'")
        elif use_ros_time:
            print(f"   ‚Ä¢ Time: ‚úÖ From ROS header timestamp")
        else:
            print(f"   ‚Ä¢ Time: ‚ùå No time information available")

        print(f"\nüìù Fields to extract: {fields_to_extract}")

        print(f"\nüîÑ PROCESSING MESSAGES...")
        print("-" * 50)

        # Reset bag reading
        bag.close()
        print("üîÑ Reopening bag file for processing...")
        bag = rosbag.Bag(bag_file, 'r')

        # Initialize progress tracking and transformation data
        processed_points = 0
        start_time = time.time()
        print(f"‚è±Ô∏è  Processing started at {time.strftime('%H:%M:%S')}")
        
        # Store message timestamps for transformation
        message_timestamps = []
        message_points_data = []
        
        # First pass: collect all data and timestamps
        print(f"üîÑ First pass: collecting point cloud data...")
        
        for _, msg, ros_timestamp in bag.read_messages(topics=[pointcloud2_topic]):
            # Increment the counter for processed messages
            message_count += 1
            
            # Calculate message timestamp for ROS time usage
            message_time = ros_timestamp.to_sec()
            message_timestamps.append(message_time)
            
            # Debug message structure on first few messages
            if message_count <= 3:
                print(f"\nüîç DEBUG Message {message_count}:")
                print(f"   ‚Ä¢ Width: {msg.width}, Height: {msg.height}")
                print(f"   ‚Ä¢ Point step: {msg.point_step}, Row step: {msg.row_step}")
                print(f"   ‚Ä¢ Expected points: {msg.width * msg.height}")
                print(f"   ‚Ä¢ Data length: {len(msg.data)} bytes")
                print(f"   ‚Ä¢ Is dense: {msg.is_dense}")
                print(f"   ‚Ä¢ Message timestamp: {message_time:.6f}")
                
            # Count points in this message first to calculate time increments
            points_in_message = []
            points_read = 0
            
            try:
                for point in pc2.read_points(msg, field_names=fields_to_extract, skip_nans=True):
                    points_in_message.append(point)
                    points_read += 1
                    
                    # Debug first few points of first message
                    if message_count <= 2 and points_read <= 5:
                        print(f"      Point {points_read}: {[f'{x:.6f}' if isinstance(x, float) else x for x in point]}")
                        
            except Exception as e:
                print(f"   ‚ùå Error reading points from message {message_count}: {e}")
                continue
            
            message_points_data.append(points_in_message)
            message_points = len(points_in_message)
            
            if message_count <= 3:
                print(f"   ‚Ä¢ Actually read points: {message_points}")
                print(f"   ‚Ä¢ Points extraction ratio: {message_points / (msg.width * msg.height) * 100:.1f}%")
                print(f"   ‚Ä¢ Point data sample: {points_in_message[:3]}")
            
            # Enhanced progress indicator with percentage bar
            progress = (message_count / total_messages) * 100
            elapsed_time = time.time() - start_time
            
            # Show progress more frequently for small files, less for large files
            if total_messages <= 10:
                show_progress = True
            elif total_messages <= 100:
                show_progress = (message_count % 5 == 0) or (message_count <= 3) or (message_count == total_messages)
            else:
                show_progress = (message_count % max(1, total_messages // 20) == 0) or (message_count <= 3) or (message_count == total_messages)
            
            if show_progress:
                # Calculate ETA
                if message_count > 1:
                    avg_time_per_msg = elapsed_time / message_count
                    eta_seconds = (total_messages - message_count) * avg_time_per_msg
                    eta_str = f"ETA: {int(eta_seconds//60):02d}:{int(eta_seconds%60):02d}"
                else:
                    eta_str = "ETA: --:--"
                
                # Create progress bar
                bar_length = 30
                filled_length = int(bar_length * progress / 100)
                bar = "‚ñà" * filled_length + "‚ñë" * (bar_length - filled_length)
                
                print(f"   üìä [{bar}] {progress:5.1f}% | Msg {message_count:4d}/{total_messages} | {message_points:,} pts | {eta_str}")
                
                # Show points processing rate
                if processed_points > 0 and elapsed_time > 0:
                    points_per_sec = processed_points / elapsed_time
                    print(f"       ‚ö° Processing rate: {points_per_sec:,.0f} points/sec | Elapsed: {int(elapsed_time//60):02d}:{int(elapsed_time%60):02d}")
            else:
                print(f"   üìä [{progress:5.1f}%] Processing message {message_count:4d}/{total_messages} - {message_points:,} points")

            points_per_message.append(message_points)
            total_points += message_points
            processed_points += message_points

        # Final progress update
        final_elapsed = time.time() - start_time
        print(f"\n   üèÅ Data collection completed in {int(final_elapsed//60):02d}:{int(final_elapsed%60):02d}")
        print(f"   ‚ö° Average processing rate: {total_points/final_elapsed:,.0f} points/sec")
        
        # Prepare transformation if needed
        reference_transform = None
        if transform_mode in ["global", "local"] and odom_timestamps is not None:
            print(f"\nüîß –ü–û–î–ì–û–¢–û–í–ö–ê –¢–†–ê–ù–°–§–û–†–ú–ê–¶–ò–ò...")
            
            # Convert message timestamps to numpy array
            message_timestamps = np.array(message_timestamps)
            
            # Interpolate odometry data to match message timestamps
            interp_positions, interp_orientations = interpolate_odometry_data(
                message_timestamps, odom_timestamps, odom_positions, odom_orientations
            )
            
            if interp_positions is None:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –æ–¥–æ–º–µ—Ç—Ä–∏–∏, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–µ–∂–∏–º –±–µ–∑ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏")
                transform_mode = "none"
            else:
                print(f"   ‚úÖ –î–∞–Ω–Ω—ã–µ –æ–¥–æ–º–µ—Ç—Ä–∏–∏ –∏–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è {len(message_timestamps)} —Å–æ–æ–±—â–µ–Ω–∏–π")
                
                # For local mode, calculate reference transform from first message
                if transform_mode == "local":
                    reference_transform = create_transform_matrix(
                        interp_positions[0], interp_orientations[0]
                    )
                    # Invert reference transform to get transformation TO reference frame
                    reference_transform = np.linalg.inv(reference_transform)
                    print(f"   üéØ –õ–æ–∫–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–≤–æ–≥–æ —Å–∫–∞–Ω–∞")
        
        # Second pass: process points with transformation
        print(f"\nüîÑ Second pass: processing and transforming points...")
        start_time = time.time()
        
        # Calculate time increment per point if using ROS time
        time_increment = 0
        if use_ros_time and len(message_points_data) > 1:
            # Option 1: Use message timestamp for all points (uniform coloring per scan)
            # Option 2: Create sequential time across all points
            # We'll use Option 1 for now - all points in message get same timestamp
            time_increment = 0  # No increment within message
            print(f"   ‚è±Ô∏è  Using uniform timestamp per message for better visualization")

        # Process each message with its interpolated transform
        for msg_idx, points_in_message in enumerate(message_points_data):
            message_time = message_timestamps[msg_idx]
            
            # Get transformation matrix for this message
            transform_matrix = None
            if transform_mode == "global" and interp_positions is not None:
                transform_matrix = create_transform_matrix(
                    interp_positions[msg_idx], interp_orientations[msg_idx]
                )
            elif transform_mode == "local" and interp_positions is not None:
                msg_transform = create_transform_matrix(
                    interp_positions[msg_idx], interp_orientations[msg_idx]
                )
                # Transform relative to reference (first scan)
                transform_matrix = reference_transform @ msg_transform
            
            # Extract points for this message
            msg_points = []
            for point_idx, point in enumerate(points_in_message):
                # Debug first 3 points of first message
                if msg_idx == 0 and point_idx < 3:
                    print(f"      Point {point_idx}: {[f'{x:.6f}' if isinstance(x, float) else x for x in point]}")
                
                # Extract coordinates
                point_coords = np.array([point[0], point[1], point[2]])
                msg_points.append(point_coords)
                
                # Store other fields for later use
                field_idx = 3
                if has_intensity:
                    intensity_val = point[field_idx] if len(point) > field_idx else 0
                    intensity_list.append(intensity_val)
                    if msg_idx == 0 and point_idx < 3:
                        print(f"         ‚îî‚îÄ Intensity at index {field_idx}: {intensity_val}")
                    field_idx += 1
                    
                if has_pointcloud_time:
                    time_val = point[field_idx] if len(point) > field_idx else 0
                    gps_time_list.append(time_val)
                    if msg_idx == 0 and point_idx < 3:
                        print(f"         ‚îî‚îÄ PointCloud time at index {field_idx}: {time_val}")
                elif use_ros_time:
                    # Use message timestamp for all points in the message
                    # This creates clear temporal separation between scans
                    gps_time_list.append(message_time)
                    if msg_idx == 0 and point_idx < 3:
                        print(f"         ‚îî‚îÄ ROS time: {message_time:.6f} (uniform for message)")
            
            # Apply transformation to all points in this message if needed
            if len(msg_points) > 0:
                msg_points = np.array(msg_points)
                
                if transform_matrix is not None:
                    if msg_idx == 0:
                        print(f"   üîß –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∫ —Ç–æ—á–∫–∞–º...")
                        print(f"      ‚Ä¢ –†–µ–∂–∏–º: {transform_mode}")
                        if transform_mode == "global":
                            print(f"      ‚Ä¢ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ –≥–ª–æ–±–∞–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç")
                        elif transform_mode == "local":
                            print(f"      ‚Ä¢ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–≤–æ–≥–æ —Å–∫–∞–Ω–∞")
                    
                    # Store original points for validation
                    original_msg_points = msg_points.copy() if msg_idx == 0 else None
                    
                    # Apply transformation
                    msg_points = apply_transform_to_points(msg_points, transform_matrix)
                    
                    if msg_idx == 0:
                        print(f"      ‚úÖ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ –∫ {len(msg_points)} —Ç–æ—á–∫–∞–º")
                        # Show transformation result for first few points
                        for i in range(min(3, len(msg_points))):
                            print(f"         Point {i} after transform: ({msg_points[i, 0]:.6f}, {msg_points[i, 1]:.6f}, {msg_points[i, 2]:.6f})")
                        
                        # Validate transformation on first message
                        if original_msg_points is not None:
                            validate_coordinate_systems(
                                original_msg_points, 
                                msg_points, 
                                f"{transform_mode} transformation"
                            )
                
                # Add transformed points to global lists
                x_list.extend(msg_points[:, 0])
                y_list.extend(msg_points[:, 1])
                z_list.extend(msg_points[:, 2])
            
            # Progress indicator
            if (msg_idx + 1) % max(1, len(message_points_data) // 20) == 0 or msg_idx < 3:
                progress = ((msg_idx + 1) / len(message_points_data)) * 100
                elapsed = time.time() - start_time
                print(f"   üìä [{progress:5.1f}%] Processed message {msg_idx + 1}/{len(message_points_data)} - {len(msg_points)} points")

        # Close the bag file
        bag.close()

        # Convert lists to numpy arrays
        print(f"\nüî¢ CONVERTING TO NUMPY ARRAYS...")
        conversion_start = time.time()
        
        print("   üîÑ Converting X coordinates...")
        x_array = np.array(x_list, dtype=np.float64)
        print(f"   ‚úÖ X coordinates converted ({len(x_array):,} points)")
        print(f"   üîç Debug - X array stats: min={np.min(x_array):.6f}, max={np.max(x_array):.6f}")
        
        print("   üîÑ Converting Y coordinates...")
        y_array = np.array(y_list, dtype=np.float64)
        print(f"   ‚úÖ Y coordinates converted ({len(y_array):,} points)")
        print(f"   üîç Debug - Y array stats: min={np.min(y_array):.6f}, max={np.max(y_array):.6f}")
        
        print("   üîÑ Converting Z coordinates...")
        z_array = np.array(z_list, dtype=np.float64)
        print(f"   ‚úÖ Z coordinates converted ({len(z_array):,} points)")
        print(f"   üîç Debug - Z array stats: min={np.min(z_array):.6f}, max={np.max(z_array):.6f}")
        
        # Add debug info about original lists before conversion
        print(f"   üîç Debug - Original list sizes: x={len(x_list)}, y={len(y_list)}, z={len(z_list)}")
        print(f"   üîç Debug - First 3 points from lists:")
        for i in range(min(3, len(x_list))):
            print(f"      Point {i}: ({x_list[i]:.6f}, {y_list[i]:.6f}, {z_list[i]:.6f})")
        
        if has_intensity:
            print("   üîÑ Converting intensity values...")
            intensity_array = np.array(intensity_list, dtype=np.float32)
            print(f"   ‚úÖ Intensity values converted ({len(intensity_array):,} points)")
            
        if has_gps_time:
            print("   üîÑ Converting GPS time values...")
            gps_time_array = np.array(gps_time_list, dtype=np.float64)
            print(f"   ‚úÖ GPS time values converted ({len(gps_time_array):,} points)")

        conversion_time = time.time() - conversion_start
        print(f"   ‚è±Ô∏è  Array conversion completed in {conversion_time:.2f} seconds")
        print(f"   üíæ Memory usage: ~{(len(x_array) * 3 * 8) / 1024 / 1024:.1f} MB for coordinates")
        
        # Data quality analysis
        print(f"\nüîç DATA QUALITY ANALYSIS:")
        print("-" * 50)
        
        # Coordinate statistics
        print(f"üìç Coordinate ranges:")
        print(f"   ‚Ä¢ X: {np.min(x_array):12.6f} to {np.max(x_array):12.6f} (range: {np.max(x_array) - np.min(x_array):10.6f})")
        print(f"   ‚Ä¢ Y: {np.min(y_array):12.6f} to {np.max(y_array):12.6f} (range: {np.max(y_array) - np.min(y_array):10.6f})")
        print(f"   ‚Ä¢ Z: {np.min(z_array):12.6f} to {np.max(z_array):12.6f} (range: {np.max(z_array) - np.min(z_array):10.6f})")
        
        # Check for valid coordinates
        nan_x = np.isnan(x_array).sum()
        nan_y = np.isnan(y_array).sum()
        nan_z = np.isnan(z_array).sum()
        inf_x = np.isinf(x_array).sum()
        inf_y = np.isinf(y_array).sum()
        inf_z = np.isinf(z_array).sum()
        
        print(f"\nüß™ Data integrity:")
        print(f"   ‚Ä¢ NaN values - X: {nan_x}, Y: {nan_y}, Z: {nan_z}")
        print(f"   ‚Ä¢ Inf values - X: {inf_x}, Y: {inf_y}, Z: {inf_z}")
        print(f"   ‚Ä¢ Zero values - X: {np.sum(x_array == 0)}, Y: {np.sum(y_array == 0)}, Z: {np.sum(z_array == 0)}")
        print(f"   ‚Ä¢ Total points before filtering: {len(x_array):,}")
        
        # More comprehensive filtering
        valid_mask = ~(np.isnan(x_array) | np.isnan(y_array) | np.isnan(z_array) |
                      np.isinf(x_array) | np.isinf(y_array) | np.isinf(z_array))
        
        # Additional check for extremely small values that might be precision errors
        extremely_small_threshold = 1e-10
        too_small_x = np.abs(x_array) < extremely_small_threshold
        too_small_y = np.abs(y_array) < extremely_small_threshold  
        too_small_z = np.abs(z_array) < extremely_small_threshold
        
        print(f"   ‚Ä¢ Extremely small values - X: {np.sum(too_small_x)}, Y: {np.sum(too_small_y)}, Z: {np.sum(too_small_z)}")
        
        # Check for points at origin (might be invalid)
        origin_points = (np.abs(x_array) < 1e-6) & (np.abs(y_array) < 1e-6) & (np.abs(z_array) < 1e-6)
        print(f"   ‚Ä¢ Points near origin (0,0,0): {np.sum(origin_points)}")
        
        if nan_x + nan_y + nan_z + inf_x + inf_y + inf_z > 0:
            print("‚ö†Ô∏è  WARNING: Invalid values detected, filtering...")
            x_array = x_array[valid_mask]
            y_array = y_array[valid_mask]
            z_array = z_array[valid_mask]
            if has_intensity:
                intensity_array = intensity_array[valid_mask]
            if has_gps_time:
                gps_time_array = gps_time_array[valid_mask]
            print(f"‚úÖ Valid points after filtering: {len(x_array):,}")
        else:
            print("‚úÖ All coordinate values are valid")
        
        if len(x_array) == 0:
            print("‚ùå ERROR: No valid points remaining after filtering")
            return

        print(f"\nüìÅ CREATING LAS FILE...")
        print("-" * 50)
        
        # Extract the base filename from the bag file path
        base_filename = os.path.splitext(os.path.basename(bag_file))[0]

        # Create the output directory if it doesn't exist
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            print(f"üìÇ Created output directory: {output_dir}")

        # Create LAS file (uncompressed)
        output_file = os.path.join(output_dir, base_filename + ".las")
        print(f"üéØ Output file: {output_file}")
        
        # Choose appropriate point format based on available fields (NO RGB)
        # Point format 0: XYZ only
        # Point format 1: XYZ + GPS time + intensity
        # Point format 4: XYZ + GPS time (no intensity, no RGB)
        # Point format 6: XYZ + GPS time (LAS 1.4, no intensity, no RGB)
        if has_gps_time and has_intensity:
            point_format = 1  # GPS time + intensity (no RGB)
        elif has_gps_time:
            point_format = 6  # GPS time only (LAS 1.4, no RGB)
        elif has_intensity:
            point_format = 0  # Basic format with intensity handled as extra dimension
        else:
            point_format = 0  # Basic format
        
        print(f"üîß Using point format: {point_format}")
        
        # Display format capabilities
        format_capabilities = {
            0: "XYZ",
            1: "XYZ + GPS time + intensity",
            4: "XYZ + GPS time",
            6: "XYZ + GPS time (LAS 1.4)"
        }
        print(f"   üìã Format {point_format} supports: {format_capabilities.get(point_format, 'Unknown')}")
        
        out_las = laspy.create(file_version='1.4', point_format=point_format)
        
        print(f"‚öôÔ∏è  Setting coordinate data...")
        # Set X, Y, Z coordinates
        out_las.x = x_array
        out_las.y = y_array
        out_las.z = z_array
        
        # Set intensity if available and supported by format
        if has_intensity:
            if point_format in [1]:  # Only format 1 officially supports intensity
                print(f"‚öôÔ∏è  Processing intensity data...")
                # Keep intensity in original format
                max_intensity = np.max(intensity_array)
                min_intensity = np.min(intensity_array)
                print(f"   ‚Ä¢ Original intensity range: {min_intensity:.3f} to {max_intensity:.3f}")
                print(f"   ‚Ä¢ Unique intensity values: {len(np.unique(intensity_array)):,}")
                
                # Use intensity as-is without scaling
                out_las.intensity = intensity_array.astype(np.uint16)
                print(f"   ‚Ä¢ Intensity used as-is (original values)")
                print(f"   ‚úÖ Intensity data added successfully")
            else:
                print(f"   ‚ö†Ô∏è  WARNING: Point format {point_format} doesn't support intensity field")
                print(f"   ‚ÑπÔ∏è  Intensity data will be lost in this format")
        
        # Set GPS time if available
        if has_gps_time:
            print(f"‚öôÔ∏è  Processing GPS time data...")
            print(f"   ‚Ä¢ GPS time array length: {len(gps_time_array):,}")
            print(f"   ‚Ä¢ GPS time data type: {gps_time_array.dtype}")
            print(f"   ‚Ä¢ GPS time range: {np.min(gps_time_array):.6f} to {np.max(gps_time_array):.6f}")
            print(f"   ‚Ä¢ Unique GPS time values: {len(np.unique(gps_time_array)):,}")
            
            # Alternative: Create sequential time for better visualization
            if use_ros_time:
                print(f"   üé® Creating sequential time for better visualization...")
                # Create linearly increasing time across all points
                total_duration = np.max(gps_time_array) - np.min(gps_time_array)
                if total_duration > 0:
                    # Create smooth time progression
                    sequential_time = np.linspace(
                        np.min(gps_time_array), 
                        np.max(gps_time_array), 
                        len(gps_time_array)
                    )
                    gps_time_array = sequential_time
                    print(f"   ‚Ä¢ Sequential time created: {np.min(gps_time_array):.6f} to {np.max(gps_time_array):.6f}")
                    print(f"   ‚Ä¢ Time step: {(np.max(gps_time_array) - np.min(gps_time_array)) / len(gps_time_array):.6f} s")
                else:
                    # If all timestamps are the same, create artificial progression
                    base_time = gps_time_array[0]
                    duration = len(points_per_message) * 0.1  # 0.1 seconds per message
                    sequential_time = np.linspace(base_time, base_time + duration, len(gps_time_array))
                    gps_time_array = sequential_time
                    print(f"   ‚Ä¢ Artificial time progression created over {duration:.1f} seconds")
            
            # More detailed time analysis
            time_diffs = np.diff(gps_time_array)
            non_zero_diffs = time_diffs[time_diffs > 0]
            if len(non_zero_diffs) > 0:
                print(f"   ‚Ä¢ Min time difference: {np.min(non_zero_diffs):.6f} s")
                print(f"   ‚Ä¢ Max time difference: {np.max(non_zero_diffs):.6f} s")
                print(f"   ‚Ä¢ Average time difference: {np.mean(non_zero_diffs):.6f} s")
                print(f"   ‚Ä¢ Median time difference: {np.median(non_zero_diffs):.6f} s")
            
            # Check for time progression
            monotonic_increasing = np.all(np.diff(gps_time_array) >= 0)
            print(f"   ‚Ä¢ Time is monotonically increasing: {monotonic_increasing}")
            if not monotonic_increasing:
                print("   ‚ö†Ô∏è  WARNING: Time values are not monotonically increasing")
                print("   üîÑ Sorting points by time for proper visualization...")
                # Sort all arrays by time
                sort_indices = np.argsort(gps_time_array)
                gps_time_array = gps_time_array[sort_indices]
                x_array = x_array[sort_indices]
                y_array = y_array[sort_indices]
                z_array = z_array[sort_indices]
                if has_intensity:
                    intensity_array = intensity_array[sort_indices]
                print("   ‚úÖ Points sorted by timestamp")
            
            print(f"   ‚Ä¢ First 10 GPS time values: {gps_time_array[:10]}")
            print(f"   ‚Ä¢ Last 10 GPS time values: {gps_time_array[-10:]}")
            
            # Analyze GPS time characteristics
            time_diff = np.max(gps_time_array) - np.min(gps_time_array)
            print(f"   ‚Ä¢ Total time span: {time_diff:.6f} seconds ({time_diff/60:.2f} minutes)")
            
            # Visualization recommendations
            unique_times = len(np.unique(gps_time_array))
            if unique_times == len(gps_time_array):
                print("   üé® Perfect for temporal visualization - each point has unique time")
            elif unique_times > len(gps_time_array) * 0.8:
                print("   üé® Good for temporal visualization - most points have unique times")
            elif unique_times > 10:
                print("   üé® Acceptable for temporal visualization - points grouped by scan time")
            else:
                print("   ‚ö†Ô∏è  Limited temporal visualization - few unique timestamps")

        # Set GPS time if available
        if has_gps_time:
            print(f"‚öôÔ∏è  Processing GPS time data...")
            print(f"   ‚Ä¢ GPS time array length: {len(gps_time_array):,}")
            print(f"   ‚Ä¢ GPS time data type: {gps_time_array.dtype}")
            print(f"   ‚Ä¢ GPS time range: {np.min(gps_time_array):.6f} to {np.max(gps_time_array):.6f}")
            print(f"   ‚Ä¢ Unique GPS time values: {len(np.unique(gps_time_array)):,}")
            
            # More detailed time analysis
            time_diffs = np.diff(gps_time_array)
            non_zero_diffs = time_diffs[time_diffs > 0]
            if len(non_zero_diffs) > 0:
                print(f"   ‚Ä¢ Min time difference: {np.min(non_zero_diffs):.6f} s")
                print(f"   ‚Ä¢ Max time difference: {np.max(non_zero_diffs):.6f} s")
                print(f"   ‚Ä¢ Average time difference: {np.mean(non_zero_diffs):.6f} s")
                print(f"   ‚Ä¢ Median time difference: {np.median(non_zero_diffs):.6f} s")
            
            # Check for time progression
            monotonic_increasing = np.all(np.diff(gps_time_array) >= 0)
            print(f"   ‚Ä¢ Time is monotonically increasing: {monotonic_increasing}")
            if not monotonic_increasing:
                print("   ‚ö†Ô∏è  WARNING: Time values are not monotonically increasing")
            
            print(f"   ‚Ä¢ First 10 GPS time values: {gps_time_array[:10]}")
            print(f"   ‚Ä¢ Last 10 GPS time values: {gps_time_array[-10:]}")
            
            # Analyze GPS time characteristics
            time_diff = np.max(gps_time_array) - np.min(gps_time_array)
            print(f"   ‚Ä¢ Total time span: {time_diff:.6f} seconds ({time_diff/60:.2f} minutes)")
            
            # Check time distribution across messages
            if use_ros_time and len(points_per_message) > 1:
                estimated_duration = len(points_per_message) * 0.1  # assuming 10Hz
                print(f"   ‚Ä¢ Estimated scan duration: {estimated_duration:.1f} seconds")
                print(f"   ‚Ä¢ Time span vs estimated: {time_diff:.1f}s vs {estimated_duration:.1f}s")
            
            # Check if GPS time values are reasonable and normalize if needed
            if np.all(gps_time_array == 0):
                print("   ‚ö†Ô∏è  WARNING: All GPS time values are zero")
            elif np.max(gps_time_array) < 1:
                print("   ‚ö†Ô∏è  WARNING: GPS time values seem to be relative (too small)")
            elif np.min(gps_time_array) > 100:
                # Always normalize large GPS time values for CloudCompare compatibility
                print("   ‚ÑπÔ∏è  GPS time values are large - normalizing for CloudCompare compatibility")
                min_gps_time = np.min(gps_time_array)
                max_gps_time_orig = np.max(gps_time_array)
                gps_time_array = gps_time_array - min_gps_time
                print(f"   üîÑ Normalizing GPS time to relative values starting from 0...")
                print(f"   ‚Ä¢ Original range: {min_gps_time:.6f} to {max_gps_time_orig:.6f}")
                print(f"   ‚Ä¢ Normalized range: 0.0 to {np.max(gps_time_array):.6f}")
                print(f"   ‚Ä¢ Duration: {np.max(gps_time_array):.2f} seconds ({np.max(gps_time_array)/60:.2f} minutes)")
                print(f"   ‚Ä¢ This allows proper time-based filtering in CloudCompare")
            else:
                print(f"   ‚ÑπÔ∏è  GPS time values are in reasonable range (0-100)")
                print(f"   ‚Ä¢ Range: {np.min(gps_time_array):.6f} to {np.max(gps_time_array):.6f}")
            
            try:
                # Ensure GPS time is in the correct format for LAS
                if point_format in [1, 4, 6]:
                    out_las.gps_time = gps_time_array
                    print(f"   ‚úÖ GPS time data assigned to LAS file")
                    
                    # Verify the assignment worked
                    if hasattr(out_las, 'gps_time') and out_las.gps_time is not None:
                        actual_min = np.min(out_las.gps_time)
                        actual_max = np.max(out_las.gps_time)
                        actual_unique = len(np.unique(out_las.gps_time))
                        print(f"   ‚úÖ Verified GPS time in LAS: {actual_min:.6f} to {actual_max:.6f}")
                        print(f"   ‚úÖ Unique time values in LAS: {actual_unique:,}")
                        print("   ‚úÖ GPS time successfully added to LAS file")
                        
                        # Check if each point has unique time
                        if actual_unique == len(gps_time_array):
                            print("   ‚úÖ Each point has unique timestamp")
                        elif actual_unique < len(gps_time_array) * 0.5:
                            print("   ‚ö†Ô∏è  WARNING: Many points share the same timestamp")
                        else:
                            print("   ‚ÑπÔ∏è  Some points share timestamps (normal for synchronized scans)")
                    else:
                        print("   ‚ùå ERROR: GPS time field is None or doesn't exist in LAS")
                else:
                    print(f"   ‚ö†Ô∏è  WARNING: Point format {point_format} doesn't support GPS time")
            except Exception as e:
                print(f"   ‚ùå ERROR setting GPS time: {e}")
                import traceback
                traceback.print_exc()

        print(f"‚öôÔ∏è  Setting LAS header parameters...")
        # Set proper header values
        out_las.header.offset = [np.min(x_array), np.min(y_array), np.min(z_array)]
        out_las.header.scale = [0.001, 0.001, 0.001]  # 1mm precision
        print(f"   ‚Ä¢ Offset: [{out_las.header.offset[0]:.6f}, {out_las.header.offset[1]:.6f}, {out_las.header.offset[2]:.6f}]")
        print(f"   ‚Ä¢ Scale: [{out_las.header.scale[0]}, {out_las.header.scale[1]}, {out_las.header.scale[2]}]")
        
        # No RGB values - skip RGB assignment entirely
        print(f"   ‚ÑπÔ∏è  No RGB data (point cloud contains no color information)")
        
        # Save the LAS file
        print(f"\nüíæ WRITING LAS FILE...")
        write_start = time.time()
        out_las.write(output_file)
        write_time = time.time() - write_start
        print(f"   ‚è±Ô∏è  File write completed in {write_time:.2f} seconds")

        # Create .POS file from odometry data
        if odometry_topic:
            create_pos_file(bag_file, output_dir, odometry_topic)
        else:
            print("\n‚ö†Ô∏è  .POS —Ñ–∞–π–ª –Ω–µ —Å–æ–∑–¥–∞–Ω - —Ç–æ–ø–∏–∫ –æ–¥–æ–º–µ—Ç—Ä–∏–∏ –Ω–µ –≤—ã–±—Ä–∞–Ω")

        # Verify file was created and provide detailed summary
        if os.path.exists(output_file):
            file_size = os.path.getsize(output_file)
            print(f"\nüéâ SUCCESS! LAS FILE CREATED")
            print("=" * 50)
            print(f"üìÅ File: {output_file}")
            print(f"üìä Size: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)")
            print(f"üî¢ Points: {len(x_array):,}")
            print(f"üìè Point format: {point_format}")
            print(f"üóÇÔ∏è  LAS version: 1.4")
            print(f"üîß Transform mode: {transform_mode}")
            
            if transform_mode != "none":
                print(f"   üåç Point cloud transformed using odometry data")
                if transform_mode == "global":
                    print(f"   üìç Coordinates in global odometry frame")
                    if enable_slam and slam_metrics:
                        print(f"   ü§ñ SLAM optimization applied:")
                        print(f"      ‚Ä¢ Loop closures found: {slam_metrics.get('loop_closures_found', 0)}")
                        print(f"      ‚Ä¢ Average position correction: {slam_metrics.get('average_position_change', 0):.3f} m")
                        if slam_metrics.get('loop_closures_found', 0) > 0:
                            print(f"      ‚Ä¢ Drift reduction: {slam_metrics.get('original_drift', 0):.3f} ‚Üí {slam_metrics.get('optimized_drift', 0):.3f} m")
                        
                        # Create trajectory visualization if SLAM was applied
                        try:
                            print(f"\nüìä –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ SLAM...")
                            # Get original positions from loaded odometry
                            if 'odom_positions_original' in locals() and slam_metrics:
                                plot_file = visualize_trajectory_comparison(
                                    odom_positions_original, odom_positions, slam_metrics,
                                    output_dir, base_filename
                                )
                                if plot_file:
                                    print(f"   ‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {plot_file}")
                            else:
                                print(f"   ‚ö†Ô∏è  –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏")
                        except Exception as viz_error:
                            print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {viz_error}")
                elif transform_mode == "local":
                    print(f"   üìç Coordinates relative to first scan position")
            else:
                print(f"   üì° Original sensor coordinates (no transformation)")
                
            # Check for .POS file
            base_filename = os.path.splitext(os.path.basename(bag_file))[0]
            pos_file = os.path.join(output_dir, base_filename + ".pos")
            if os.path.exists(pos_file):
                pos_size = os.path.getsize(pos_file)
                print(f"üß≠ POS file: {pos_file}")
                print(f"üìä POS size: {pos_size:,} bytes")
            
            # Detailed field verification
            try:
                # Re-read the file to verify it's valid
                verify_las = laspy.read(output_file)
                print(f"\n‚úÖ FILE VERIFICATION:")
                print(f"   ‚Ä¢ Points in file: {len(verify_las.points):,}")
                print(f"   ‚Ä¢ Coordinate ranges verified:")
                print(f"     - X: {np.min(verify_las.x):.6f} to {np.max(verify_las.x):.6f}")
                print(f"     - Y: {np.min(verify_las.y):.6f} to {np.max(verify_las.y):.6f}")
                print(f"     - Z: {np.min(verify_las.z):.6f} to {np.max(verify_las.z):.6f}")
                
                if has_intensity and hasattr(verify_las, 'intensity'):
                    print(f"   ‚Ä¢ Intensity verified: {np.min(verify_las.intensity)} - {np.max(verify_las.intensity)}")
                    print(f"‚ö° Intensity: ‚úÖ Range {np.min(intensity_array):.3f} - {np.max(intensity_array):.3f}")
                
                if has_gps_time and hasattr(verify_las, 'gps_time'):
                    print(f"   ‚Ä¢ GPS time verified: {np.min(verify_las.gps_time):.6f} - {np.max(verify_las.gps_time):.6f}")
                    print(f"   ‚Ä¢ GPS time unique values: {len(np.unique(verify_las.gps_time)):,}")
                    print(f"‚è∞ GPS time: ‚úÖ Range {np.min(gps_time_array):.6f} - {np.max(gps_time_array):.6f}")
                    
                    # Check temporal distribution
                    time_span = np.max(verify_las.gps_time) - np.min(verify_las.gps_time)
                    if time_span > 0:
                        points_per_second = len(verify_las.points) / time_span
                        print(f"   ‚Ä¢ Temporal density: {points_per_second:.0f} points/second")
                        print(f"   üé® Ready for temporal visualization in CloudCompare!")
                
                print(f"   ‚úÖ LAS file is valid and ready for use")
                
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Warning: Could not verify file: {e}")
            
        else:
            print("‚ùå ERROR: LAS file was not created")

    except Exception as e:
        print(f"\n‚ùå FATAL ERROR: {e}")
        import traceback
        traceback.print_exc()

def analyze_bag_topics(bag_file):
    """
    Analyze and display all topics in the bag file with detailed information.
    
    Parameters:
        bag_file (str): Path to the ROS bag file
        
    Returns:
        dict: Dictionary with topic analysis results
    """
    print(f"\nüîç –ü–û–õ–ù–´–ô –ê–ù–ê–õ–ò–ó –¢–û–ü–ò–ö–û–í –í BAG –§–ê–ô–õ–ï")
    print("=" * 80)
    print(f"üìÇ –§–∞–π–ª: {os.path.basename(bag_file)}")
    
    try:
        with rosbag.Bag(bag_file, 'r') as bag:
            # Get bag info
            bag_info = bag.get_type_and_topic_info()
            total_topics = len(bag_info.topics)
            total_messages = bag.get_message_count()
            
            # Get bag duration
            start_time = bag.get_start_time()
            end_time = bag.get_end_time()
            duration = end_time - start_time if start_time and end_time else 0
            
            print(f"üìä –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:")
            print(f"   ‚Ä¢ –í—Å–µ–≥–æ —Ç–æ–ø–∏–∫–æ–≤: {total_topics}")
            print(f"   ‚Ä¢ –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {total_messages:,}")
            print(f"   ‚Ä¢ –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {duration:.2f} —Å–µ–∫—É–Ω–¥ ({duration/60:.2f} –º–∏–Ω—É—Ç)")
            if duration > 0:
                print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è —á–∞—Å—Ç–æ—Ç–∞: {total_messages/duration:.1f} —Å–æ–æ–±—â–µ–Ω–∏–π/—Å–µ–∫")
            print(f"   ‚Ä¢ –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time)) if start_time else 'unknown'}")
            print(f"   ‚Ä¢ –í—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time)) if end_time else 'unknown'}")
            
            # Categorize topics by type
            topic_categories = {
                'sensor_msgs/PointCloud2': [],
                'nav_msgs/Odometry': [],
                'sensor_msgs/Image': [],
                'sensor_msgs/CompressedImage': [],
                'sensor_msgs/Imu': [],
                'geometry_msgs/Twist': [],
                'tf/tfMessage': [],
                'tf2_msgs/TFMessage': [],
                'other': []
            }
            
            # Sort topics by name for consistent display
            sorted_topics = sorted(bag_info.topics.items())
            
            print(f"\nüìã –î–ï–¢–ê–õ–¨–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –ü–û –¢–û–ü–ò–ö–ê–ú:")
            print("-" * 80)
            print(f"{'‚Ññ':>3} | {'–¢–æ–ø–∏–∫':^40} | {'–¢–∏–ø':^25} | {'–°–æ–æ–±—â.':>8} | {'–ß–∞—Å—Ç–æ—Ç–∞':>8}")
            print("-" * 80)
            
            for i, (topic_name, topic_info) in enumerate(sorted_topics, 1):
                msg_type = topic_info.msg_type
                msg_count = topic_info.message_count
                frequency = topic_info.frequency
                
                # Categorize topic
                if msg_type in topic_categories:
                    topic_categories[msg_type].append(topic_name)
                else:
                    topic_categories['other'].append(topic_name)
                
                # Format frequency with None check
                freq_str = f"{frequency:.1f} Hz" if frequency is not None and frequency > 0 else "0 Hz"
                
                # Truncate long topic names for display
                display_topic = topic_name[:38] + ".." if len(topic_name) > 40 else topic_name
                display_type = msg_type.split('/')[-1][:23] + ".." if len(msg_type) > 25 else msg_type.split('/')[-1]
                
                print(f"{i:3d} | {display_topic:<40} | {display_type:<25} | {msg_count:8,} | {freq_str:>8}")
            
            print("-" * 80)
            
            # Show categorized summary
            print(f"\nüéØ –ö–ê–¢–ï–ì–û–†–ò–ò –¢–û–ü–ò–ö–û–í:")
            print("-" * 50)
            
            # PointCloud2 topics
            if topic_categories['sensor_msgs/PointCloud2']:
                print(f"üì° PointCloud2 —Ç–æ–ø–∏–∫–∏ ({len(topic_categories['sensor_msgs/PointCloud2'])}):")
                for topic in topic_categories['sensor_msgs/PointCloud2']:
                    count = bag_info.topics[topic].message_count
                    freq = bag_info.topics[topic].frequency
                    freq_display = f"{freq:.1f} Hz" if freq is not None and freq > 0 else "0 Hz"
                    print(f"   ‚Ä¢ {topic} ({count:,} —Å–æ–æ–±—â–µ–Ω–∏–π, {freq_display})")
                print()
            
            # Odometry topics
            if topic_categories['nav_msgs/Odometry']:
                print(f"üß≠ Odometry —Ç–æ–ø–∏–∫–∏ ({len(topic_categories['nav_msgs/Odometry'])}):")
                for topic in topic_categories['nav_msgs/Odometry']:
                    count = bag_info.topics[topic].message_count
                    freq = bag_info.topics[topic].frequency
                    freq_display = f"{freq:.1f} Hz" if freq is not None and freq > 0 else "0 Hz"
                    print(f"   ‚Ä¢ {topic} ({count:,} —Å–æ–æ–±—â–µ–Ω–∏–π, {freq_display})")
                print()
            
            # Image topics
            image_topics = topic_categories['sensor_msgs/Image'] + topic_categories['sensor_msgs/CompressedImage']
            if image_topics:
                print(f"üì∑ Image —Ç–æ–ø–∏–∫–∏ ({len(image_topics)}):")
                for topic in image_topics:
                    count = bag_info.topics[topic].message_count
                    freq = bag_info.topics[topic].frequency
                    freq_display = f"{freq:.1f} Hz" if freq is not None and freq > 0 else "0 Hz"
                    msg_type = bag_info.topics[topic].msg_type.split('/')[-1]
                    print(f"   ‚Ä¢ {topic} ({count:,} —Å–æ–æ–±—â–µ–Ω–∏–π, {freq_display}, {msg_type})")
                print()
            
            # IMU topics
            if topic_categories['sensor_msgs/Imu']:
                print(f"üéØ IMU —Ç–æ–ø–∏–∫–∏ ({len(topic_categories['sensor_msgs/Imu'])}):")
                for topic in topic_categories['sensor_msgs/Imu']:
                    count = bag_info.topics[topic].message_count
                    freq = bag_info.topics[topic].frequency
                    freq_display = f"{freq:.1f} Hz" if freq is not None and freq > 0 else "0 Hz"
                    print(f"   ‚Ä¢ {topic} ({count:,} —Å–æ–æ–±—â–µ–Ω–∏–π, {freq_display})")
                print()
            
            # TF topics
            tf_topics = topic_categories['tf/tfMessage'] + topic_categories['tf2_msgs/TFMessage']
            if tf_topics:
                print(f"üîó Transform —Ç–æ–ø–∏–∫–∏ ({len(tf_topics)}):")
                for topic in tf_topics:
                    count = bag_info.topics[topic].message_count
                    freq = bag_info.topics[topic].frequency
                    freq_display = f"{freq:.1f} Hz" if freq is not None and freq > 0 else "0 Hz"
                    print(f"   ‚Ä¢ {topic} ({count:,} —Å–æ–æ–±—â–µ–Ω–∏–π, {freq_display})")
                print()
            
            # Control topics
            if topic_categories['geometry_msgs/Twist']:
                print(f"üéÆ Control —Ç–æ–ø–∏–∫–∏ ({len(topic_categories['geometry_msgs/Twist'])}):")
                for topic in topic_categories['geometry_msgs/Twist']:
                    count = bag_info.topics[topic].message_count
                    freq = bag_info.topics[topic].frequency
                    freq_display = f"{freq:.1f} Hz" if freq is not None and freq > 0 else "0 Hz"
                    print(f"   ‚Ä¢ {topic} ({count:,} —Å–æ–æ–±—â–µ–Ω–∏–π, {freq_display})")
                print()
            
            # Other topics
            if topic_categories['other']:
                print(f"üì¶ –î—Ä—É–≥–∏–µ —Ç–æ–ø–∏–∫–∏ ({len(topic_categories['other'])}):")
                # Group by message type
                other_by_type = {}
                for topic in topic_categories['other']:
                    msg_type = bag_info.topics[topic].msg_type
                    if msg_type not in other_by_type:
                        other_by_type[msg_type] = []
                    other_by_type[msg_type].append(topic)
                
                for msg_type, topics in sorted(other_by_type.items()):
                    print(f"   üìÑ {msg_type}:")
                    for topic in topics:
                        count = bag_info.topics[topic].message_count
                        freq = bag_info.topics[topic].frequency
                        freq_display = f"{freq:.1f} Hz" if freq is not None and freq > 0 else "0 Hz"
                        print(f"      ‚Ä¢ {topic} ({count:,} —Å–æ–æ–±—â–µ–Ω–∏–π, {freq_display})")
                print()
            
            # Summary statistics
            pointcloud_count = len(topic_categories['sensor_msgs/PointCloud2'])
            odometry_count = len(topic_categories['nav_msgs/Odometry'])
            
            print(f"‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù:")
            print(f"   ‚Ä¢ PointCloud2 —Ç–æ–ø–∏–∫–æ–≤: {pointcloud_count}")
            print(f"   ‚Ä¢ Odometry —Ç–æ–ø–∏–∫–æ–≤: {odometry_count}")
            print(f"   ‚Ä¢ –î—Ä—É–≥–∏—Ö —Ç–æ–ø–∏–∫–æ–≤: {total_topics - pointcloud_count - odometry_count}")
            
            if pointcloud_count == 0:
                print("   ‚ö†Ô∏è  WARNING: –ù–µ—Ç PointCloud2 —Ç–æ–ø–∏–∫–æ–≤ –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏!")
            if odometry_count == 0:
                print("   ‚ö†Ô∏è  WARNING: –ù–µ—Ç Odometry —Ç–æ–ø–∏–∫–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è .POS —Ñ–∞–π–ª–∞!")
            
            return {
                'total_topics': total_topics,
                'total_messages': total_messages,
                'duration': duration,
                'pointcloud2_topics': topic_categories['sensor_msgs/PointCloud2'],
                'odometry_topics': topic_categories['nav_msgs/Odometry'],
                'topics_info': bag_info.topics
            }
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ bag —Ñ–∞–π–ª–∞: {e}")
        import traceback
        traceback.print_exc()
        return None

def get_user_choice():
    """
    –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—ã–±–æ—Ä–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: —Ñ–∞–π–ª –∏–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
    
    Returns:
        tuple: (mode, path) –≥–¥–µ mode - 'file' –∏–ª–∏ 'directory', path - –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É/–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    """
    print("\nüéØ –í–´–ë–ï–†–ò–¢–ï –†–ï–ñ–ò–ú –†–ê–ë–û–¢–´:")
    print("=" * 50)
    print("1. üìÅ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é (.bag —Ñ–∞–π–ª—ã)")
    print("2. üìÑ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ñ–∞–π–ª (.bag)")
    print("3. ‚ùå –í—ã—Ö–æ–¥")
    
    while True:
        try:
            choice = input("\n–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä (1-3): ").strip()
            
            if choice == "1":
                # –†–µ–∂–∏–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                directory = input("üìÇ –í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (–∏–ª–∏ Enter –¥–ª—è —Ç–µ–∫—É—â–µ–π): ").strip()
                if not directory:
                    directory = os.path.dirname(os.path.abspath(__file__))
                
                if not os.path.isdir(directory):
                    print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {directory}")
                    continue
                
                return "directory", directory
                
            elif choice == "2":
                # –†–µ–∂–∏–º –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
                file_path = input("üìÑ –í–≤–µ–¥–∏—Ç–µ –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ .bag —Ñ–∞–π–ª—É: ").strip()
                
                if not file_path:
                    print("‚ùå –ü—É—Ç—å –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
                    continue
                    
                if not os.path.isfile(file_path):
                    print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
                    continue
                    
                if not file_path.endswith(".bag"):
                    print("‚ùå –§–∞–π–ª –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ .bag")
                    continue
                
                return "file", file_path
                
            elif choice == "3":
                print("üëã –í—ã—Ö–æ–¥ –∏–∑ –ø—Ä–æ–≥—Ä–∞–º–º—ã")
                exit(0)
                
            else:
                print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä. –í–≤–µ–¥–∏—Ç–µ 1, 2 –∏–ª–∏ 3")
                
        except KeyboardInterrupt:
            print("\nüëã –ü—Ä–æ–≥—Ä–∞–º–º–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            exit(0)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤–≤–æ–¥–∞: {e}")

def process_single_file(bag_file_path, output_dir=None, selected_topic=None, transform_mode=None, enable_slam=None):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ .bag —Ñ–∞–π–ª–∞
    
    Parameters:
        bag_file_path (str): –ü—É—Ç—å –∫ .bag —Ñ–∞–π–ª—É
        output_dir (str): –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é - —Ä—è–¥–æ–º —Å –∏—Å—Ö–æ–¥–Ω—ã–º —Ñ–∞–π–ª–æ–º)
        selected_topic (str): –í—ã–±—Ä–∞–Ω–Ω—ã–π —Ç–æ–ø–∏–∫ (–µ—Å–ª–∏ None, –±—É–¥–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –≤—ã–±–æ—Ä)
        transform_mode (str): –†–µ–∂–∏–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ (None –¥–ª—è –≤—ã–±–æ—Ä–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º)
        enable_slam (bool): –í–∫–ª—é—á–∏—Ç—å SLAM –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é (None –¥–ª—è –≤—ã–±–æ—Ä–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º)
    """
    if output_dir is None:
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        output_dir = os.path.dirname(bag_file_path)
    
    print(f"üéØ –û–ë–†–ê–ë–û–¢–ö–ê –û–¢–î–ï–õ–¨–ù–û–ì–û –§–ê–ô–õ–ê")
    print("=" * 60)
    print(f"üìÇ –ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª: {bag_file_path}")
    print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –≤—ã–≤–æ–¥–∞: {output_dir}")
    
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
    file_size = os.path.getsize(bag_file_path)
    print(f"üìä –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size:,} –±–∞–π—Ç ({file_size/1024/1024:.1f} –ú–ë)")
    
    try:
        start_time = time.time()
        convert_bag_to_laz(bag_file_path, output_dir, selected_topic, transform_mode, enable_slam)
        elapsed_time = time.time() - start_time
        
        print(f"\nüéâ –§–ê–ô–õ –£–°–ü–ï–®–ù–û –û–ë–†–ê–ë–û–¢–ê–ù!")
        print(f"‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {int(elapsed_time//60):02d}:{int(elapsed_time%60):02d}")
        
        # –ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–µ —Ñ–∞–π–ª—ã
        base_filename = os.path.splitext(os.path.basename(bag_file_path))[0]
        output_file = os.path.join(output_dir, base_filename + ".las")
        
        if os.path.exists(output_file):
            output_size = os.path.getsize(output_file)
            print(f"üìÅ –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª: {output_file}")
            print(f"üìä –†–∞–∑–º–µ—Ä LAS: {output_size:,} –±–∞–π—Ç ({output_size/1024/1024:.1f} –ú–ë)")
            print(f"üìà –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è: {file_size/output_size:.1f}x")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û–®–ò–ë–ö–ê –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {e}")
        import traceback
        traceback.print_exc()
        return False

def process_directory(bag_directory, output_dir, selected_topic=None, transform_mode=None, enable_slam=None):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö .bag —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    
    Parameters:
        bag_directory (str): –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å .bag —Ñ–∞–π–ª–∞–º–∏
        output_dir (str): –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        selected_topic (str): –í—ã–±—Ä–∞–Ω–Ω—ã–π —Ç–æ–ø–∏–∫ –¥–ª—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ (–µ—Å–ª–∏ None, –±—É–¥–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –≤—ã–±–æ—Ä –¥–ª—è –∫–∞–∂–¥–æ–≥–æ)
        transform_mode (str): –†–µ–∂–∏–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ (None –¥–ª—è –≤—ã–±–æ—Ä–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º)
        enable_slam (bool): –í–∫–ª—é—á–∏—Ç—å SLAM –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –¥–ª—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ (None –¥–ª—è –≤—ã–±–æ—Ä–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º)
    """
    print(f"üéØ –û–ë–†–ê–ë–û–¢–ö–ê –î–ò–†–ï–ö–¢–û–†–ò–ò")
    print("=" * 60)
    print(f"üìÇ –í—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {bag_directory}")
    print(f"üìÅ –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")

    # –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ .bag —Ñ–∞–π–ª–æ–≤
    bag_files = [f for f in os.listdir(bag_directory) if f.endswith(".bag")]
    print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(bag_files)} .bag —Ñ–∞–π–ª(–æ–≤)")
    
    if not bag_files:
        print(f"‚ùå .bag —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {bag_directory}")
        return False
    
    # –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    total_size = 0
    for i, bag_file in enumerate(bag_files, 1):
        bag_path = os.path.join(bag_directory, bag_file)
        size = os.path.getsize(bag_path)
        total_size += size
        print(f"   {i:2d}. {bag_file} ({size/1024/1024:.1f} –ú–ë)")
    
    print(f"üìä –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size/1024/1024:.1f} –ú–ë")
    
    # –ï—Å–ª–∏ —Ç–æ–ø–∏–∫ –Ω–µ –≤—ã–±—Ä–∞–Ω, –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –≤—ã–±—Ä–∞—Ç—å –æ–±—â–∏–π —Ç–æ–ø–∏–∫ –¥–ª—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
    use_same_topic_for_all = False
    if selected_topic is None and len(bag_files) > 1:
        choice = input(f"\n‚ùì –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–¥–∏–Ω —Ç–æ–ø–∏–∫ –¥–ª—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤? (y/n): ").strip().lower()
        if choice in ['y', 'yes', '–¥', '–¥–∞']:
            print("üîç –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤–æ–≥–æ —Ñ–∞–π–ª–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ —Ç–æ–ø–∏–∫–∞...")
            first_bag_path = os.path.join(bag_directory, bag_files[0])
            selected_topic = choose_pointcloud2_topic(first_bag_path)
            if selected_topic:
                use_same_topic_for_all = True
                print(f"‚úÖ –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω —Ç–æ–ø–∏–∫ '{selected_topic}' –¥–ª—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤")
            else:
                print("‚ùå –¢–æ–ø–∏–∫ –Ω–µ –≤—ã–±—Ä–∞–Ω, –±—É–¥–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –≤—ã–±–æ—Ä –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞")
    
    # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    confirm = input(f"\n‚ùì –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ {len(bag_files)} —Ñ–∞–π–ª–æ–≤? (y/n): ").strip().lower()
    if confirm not in ['y', 'yes', '–¥', '–¥–∞']:
        print("‚ùå –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞")
        return False

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
    success_count = 0
    total_start_time = time.time()
    
    for i, bag_file in enumerate(bag_files, 1):
        try:
            bag_file_path = os.path.join(bag_directory, bag_file)
            file_progress = (i / len(bag_files)) * 100
            
            print(f"\nüîÑ [{file_progress:5.1f}%] –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ {i}/{len(bag_files)}: {bag_file}")
            print("=" * 60)
            
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–π —Ç–æ–ø–∏–∫ –∏–ª–∏ –¥–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤—ã–±—Ä–∞—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
            topic_for_this_file = selected_topic if use_same_topic_for_all else None
            
            file_start_time = time.time()
            convert_bag_to_laz(bag_file_path, output_dir, topic_for_this_file, transform_mode, enable_slam)
            file_elapsed = time.time() - file_start_time
            
            print(f"\n‚úÖ –§–∞–π–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {int(file_elapsed//60):02d}:{int(file_elapsed%60):02d}")
            success_count += 1
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {bag_file}: {e}")
    
    total_elapsed = time.time() - total_start_time
    print(f"\nüèÅ –û–ë–†–ê–ë–û–¢–ö–ê –î–ò–†–ï–ö–¢–û–†–ò–ò –ó–ê–í–ï–†–®–ï–ù–ê!")
    print("=" * 80)
    print(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {int(total_elapsed//60):02d}:{int(total_elapsed%60):02d}")
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {success_count}/{len(bag_files)} —Ñ–∞–π–ª–æ–≤")
    
    if success_count < len(bag_files):
        print(f"‚ùå –û—à–∏–±–æ–∫: {len(bag_files) - success_count} —Ñ–∞–π–ª–æ–≤")
    
    if success_count > 0:
        avg_time_per_file = total_elapsed / success_count
        print(f"üìä –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —Ñ–∞–π–ª: {avg_time_per_file:.1f} —Å–µ–∫—É–Ω–¥")
    
    return success_count > 0

if __name__ == "__main__":
    import time  # Add time import for progress tracking
    
    print("üöÄ –ö–û–ù–í–ï–†–¢–ï–† ROS BAG –í LAS")
    print("=" * 80)
    print("–í–µ—Ä—Å–∏—è 2.0 - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –≤—ã–±–æ—Ä–∞ —Ñ–∞–π–ª–æ–≤ –∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π")
    
    try:
        # –ü–æ–ª—É—á–∏—Ç—å –≤—ã–±–æ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        mode, path = get_user_choice()
        
        if mode == "file":
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            output_dir = input(f"\nüìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (Enter –¥–ª—è '{os.path.dirname(path)}'): ").strip()
            if not output_dir:
                output_dir = os.path.dirname(path)
            
            # –°–æ–∑–¥–∞—Ç—å –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)
                print(f"üìÇ –°–æ–∑–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")
            
            success = process_single_file(path, output_dir)
            
        elif mode == "directory":
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            output_dir = input(f"\nüìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (Enter –¥–ª—è '{path}'): ").strip()
            if not output_dir:
                output_dir = path
            
            # –°–æ–∑–¥–∞—Ç—å –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)
                print(f"üìÇ –°–æ–∑–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")
            
            success = process_directory(path, output_dir)
        
        if success:
            print(f"\nüéâ –ü–†–û–ì–†–ê–ú–ú–ê –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
        else:
            print(f"\n‚ùå –ü–†–û–ì–†–ê–ú–ú–ê –ó–ê–í–ï–†–®–ï–ù–ê –° –û–®–ò–ë–ö–ê–ú–ò")
            
    except KeyboardInterrupt:
        print(f"\nüëã –ü—Ä–æ–≥—Ä–∞–º–º–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()
        # else:
        #     print(f"\n‚ùå –ü–†–û–ì–†–ê–ú–ú–ê –ó–ê–í–ï–†–®–ï–ù–ê –° –û–®–ò–ë–ö–ê–ú–ò")
            
    except KeyboardInterrupt:
        print(f"\nüëã –ü—Ä–æ–≥—Ä–∞–º–º–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()

